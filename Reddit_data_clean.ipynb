{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit_data_clean.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gosbLEJ-i03f",
        "QFdadyGWkMIV",
        "BVP8GKXskitI",
        "GDhrTvZjk5LJ",
        "ud9Dy7TcleC9",
        "IMnHcBs5lhFv",
        "AKuReBoHmUjc",
        "PnuAkOLinU3I",
        "koXHKZkEmWXV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7HrzWHv0Xns",
        "colab_type": "code",
        "outputId": "a87c0902-504c-43eb-9682-97405ab6ce14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DSVINrXsKf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "610e3285-319e-4397-e5d3-674ad6570022"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''<script>\n",
        "code_show_err=false; \n",
        "function code_toggle_err() {\n",
        " if (code_show_err){\n",
        " $('div.output_stderr').hide();\n",
        " } else {\n",
        " $('div.output_stderr').show();\n",
        " }\n",
        " code_show_err = !code_show_err\n",
        "} \n",
        "$( document ).ready(code_toggle_err);\n",
        "</script>\n",
        "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<script>\n",
              "code_show_err=false; \n",
              "function code_toggle_err() {\n",
              " if (code_show_err){\n",
              " $('div.output_stderr').hide();\n",
              " } else {\n",
              " $('div.output_stderr').show();\n",
              " }\n",
              " code_show_err = !code_show_err\n",
              "} \n",
              "$( document ).ready(code_toggle_err);\n",
              "</script>\n",
              "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLoiaK4U0ssB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Data\n",
        "data = pd.read_csv('/content/drive/My Drive/reddit-data-cleaned.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEylSp_l2k6-",
        "colab_type": "code",
        "outputId": "af408780-0126-4c00-a9e9-144699a48ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "      <th>body</th>\n",
              "      <th>url</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>comments</th>\n",
              "      <th>flair</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>delhi govt source names cm arvind kejriwal dep...</td>\n",
              "      <td>302</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ani status</td>\n",
              "      <td>30</td>\n",
              "      <td>beyond petty inclusion delhi government school...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>f7ogd8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delhi ap singh advocate delhi gang rape convic...</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ani status</td>\n",
              "      <td>22</td>\n",
              "      <td>hunch guy try expose loophole legal system nev...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>flgvah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>supreme court verdict sc st quota create polit...</td>\n",
              "      <td>106</td>\n",
              "      <td>NaN</td>\n",
              "      <td>scroll article supreme courts verdict sc st qu...</td>\n",
              "      <td>47</td>\n",
              "      <td>muslim reservation two distraction use indian ...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>f1o839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>entrance exam schedule may</td>\n",
              "      <td>9</td>\n",
              "      <td>clat ailet neet jee postpone two week would ab...</td>\n",
              "      <td>india comments fvcvo entrance exams scheduled may</td>\n",
              "      <td>3</td>\n",
              "      <td>bachega india tabhi toh padhega india gand mar...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>fvcvo1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>advisory schedule international mercial passen...</td>\n",
              "      <td>36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>pib india status</td>\n",
              "      <td>4</td>\n",
              "      <td>oh boy chalo bhaisahab sabji ka dukaan main da...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>fl8zf5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  score  ...      flair      id\n",
              "0  delhi govt source names cm arvind kejriwal dep...    302  ...  Scheduled  f7ogd8\n",
              "1  delhi ap singh advocate delhi gang rape convic...     17  ...  Scheduled  flgvah\n",
              "2  supreme court verdict sc st quota create polit...    106  ...  Scheduled  f1o839\n",
              "3                         entrance exam schedule may      9  ...  Scheduled  fvcvo1\n",
              "4  advisory schedule international mercial passen...     36  ...  Scheduled  fl8zf5\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsVoYfNSijYu",
        "colab_type": "text"
      },
      "source": [
        "## **1. Bag of words Model on conventional ML algorithms**\n",
        "---\n",
        "- We cannot input text directly to machine learning models. We need to convert the text to a vector of numbers, this step is called **Feature extraction**\n",
        "  \n",
        "    \n",
        "- For this we are going to use B.O.W (Bag of words) model, It focuses only on the occurence of words. The sentence structure, context, order of words is lost in B.O.W model.\n",
        "\n",
        "\n",
        "- First we will convert each document in corpus to TF-IDF vector\n",
        "\n",
        "  \n",
        "- We will input these vectors to Machine Learning models like Naive-Bayes, Support-Vector-Machine, Logistic-Regression, Random-Forest \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gosbLEJ-i03f",
        "colab_type": "text"
      },
      "source": [
        "### **Preparing data for model**\n",
        "---\n",
        "\n",
        "- It is possible that some features from the dataset may perform better than others. For instance, Only using **Title** for our model may give better accuracy than using only **url**, or maybe a combination of such features might result in a better accuracy. \n",
        "\n",
        "  \n",
        "- this is hard to guess at the moment, so I plan to try out different combinations of inputs from the dataset to get the best accuracy:- Title, url, comments, (Title + url + comments), (Title + url) , etc.\n",
        "\n",
        "  \n",
        "- lets see which performs the best, we will use those features in our final model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_RFjrcViiwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(columns):\n",
        "    # Prepares Train and test sets for models\n",
        "        \n",
        "    if len(columns) > 2:\n",
        "        df = data[columns].fillna(\"\")\n",
        "        columns.remove('flair')\n",
        "        X = df[columns].apply(lambda x : ' '.join(x),axis = 1)                     \n",
        "       \n",
        "    else :\n",
        "        df = data[columns].dropna()\n",
        "        X = df[columns[0]]\n",
        "        \n",
        "    X = X.values          # X - input\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    Y = le.fit_transform(df['flair'])    # Y - target_labels\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)   # ( 85 : 15 )\n",
        "    return (X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFdadyGWkMIV",
        "colab_type": "text"
      },
      "source": [
        "### **1.1 Title**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-_gyjAAiito",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d8b1193b-1277-4a2c-aaf2-29dfbbb0ee4c"
      },
      "source": [
        "# Extracting 'Title' from dataset\n",
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVP8GKXskitI",
        "colab_type": "text"
      },
      "source": [
        "### Data is ready now we'll apply it to different classifiers\n",
        "---\n",
        "1. Linear SVC\n",
        "2. Naive - Bayes\n",
        "3. Logistic Regression\n",
        "4. Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fenFYNOiiiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5c387b22-fc2d-4b65-8ae2-a1017cf8d59c"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.2)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 0.9)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100,max_depth=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        # tf-idf vectorisation\n",
        "                    (clf[0],clf[1])                                            # estimator\n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7194029850746269\n",
            "\n",
            "Naive - Bayes   ----->   0.7074626865671642\n",
            "\n",
            "LogisticRegression   ----->   0.746268656716418\n",
            "\n",
            "Random Forest Classifier   ----->   0.7134328358208956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDhrTvZjk5LJ",
        "colab_type": "text"
      },
      "source": [
        "### **1.2 URL** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqjUYJd3iiZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "02951045-0347-43e0-bc41-7886a06de2cf"
      },
      "source": [
        "# Extracting url column from dataset\n",
        "(X_train, X_test, y_train, y_test) = prepare_data(['url','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1890,)\n",
            "y_train shape  =  (1890,)\n",
            "X_test shape  =  (334,)\n",
            "y_test shape  =  (334,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daogzFoPiiOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "71317f51-8b4e-485c-f193-4a07225e75f4"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 25,solver='saga',penalty='l1',multi_class='multinomial')) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100,max_depth=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.5269461077844312\n",
            "\n",
            "Naive - Bayes   ----->   0.4431137724550898\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression   ----->   0.5239520958083832\n",
            "\n",
            "Random Forest Classifier   ----->   0.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud9Dy7TcleC9",
        "colab_type": "text"
      },
      "source": [
        "### **1.3 Comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGUvrfmflgaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dac06099-7b23-4a4f-8546-ec0fbbf88513"
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['comments','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1706,)\n",
            "y_train shape  =  (1706,)\n",
            "X_test shape  =  (302,)\n",
            "y_test shape  =  (302,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwCy8srNlgHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "efe8703d-b2b4-4755-9e4d-56b5969672dc"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=1)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 10)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.5132450331125827\n",
            "\n",
            "Naive - Bayes   ----->   0.4105960264900662\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression   ----->   0.49337748344370863\n",
            "\n",
            "Random Forest Classifier   ----->   0.423841059602649\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMnHcBs5lhFv",
        "colab_type": "text"
      },
      "source": [
        "### **1.4 Title + url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjlMDQXBmTy7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "febffe9e-5518-4c27-b366-dee5e9b8c20c"
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','url','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFJ3In98mThE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5e23c5aa-1fc9-4996-bcb2-4f0d74fc3fb5"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 0.9)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7283582089552239\n",
            "\n",
            "Naive - Bayes   ----->   0.6925373134328359\n",
            "\n",
            "LogisticRegression   ----->   0.7283582089552239\n",
            "\n",
            "Random Forest Classifier   ----->   0.7074626865671642\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKuReBoHmUjc",
        "colab_type": "text"
      },
      "source": [
        "### **1.5 Title + comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5hT1sUmVe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2649ae2a-ad1f-4e91-841d-fbf6ceec0157"
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','comments','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMRlPaffmV15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "517e4d75-8588-40a5-daca-23ccab31d93f"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.6)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 150)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7432835820895523\n",
            "\n",
            "Naive - Bayes   ----->   0.5671641791044776\n",
            "\n",
            "LogisticRegression   ----->   0.7194029850746269\n",
            "\n",
            "Random Forest Classifier   ----->   0.7134328358208956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnuAkOLinU3I",
        "colab_type": "text"
      },
      "source": [
        "### **1.6 Title + body**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxvKsET6nVp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "26182970-6726-4208-f92f-6c36107ce639"
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','body','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxUjKI29nWGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b25f2630-9c4a-4edc-88e8-2fad3ea6bb57"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 200)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.8238805970149253\n",
            "\n",
            "Naive - Bayes   ----->   0.6716417910447762\n",
            "\n",
            "LogisticRegression   ----->   0.7970149253731343\n",
            "\n",
            "Random Forest Classifier   ----->   0.7611940298507462\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koXHKZkEmWXV",
        "colab_type": "text"
      },
      "source": [
        "### **1.7 Title + comments + url**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKrWvk4WnUWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d0decdbe-eadb-4bcf-f65c-af21bcc1c4d9"
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','url','comments','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNUcG1lVnUAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b056ef93-02a1-4896-8370-3a6272138f2e"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=3)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 150)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7761194029850746\n",
            "\n",
            "Naive - Bayes   ----->   0.5940298507462687\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression   ----->   0.7343283582089553\n",
            "\n",
            "Random Forest Classifier   ----->   0.7134328358208956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ3bcnlxnWnj",
        "colab_type": "text"
      },
      "source": [
        "###  **1.8 Title + body + url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5FeEtE7oIhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a9172c2-5813-4c2a-e9b1-3406493f011d"
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','body','url','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WahGxyEKoIMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "683b3743-a321-4db7-a134-44be95413395"
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 200)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test) \n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test) # t = b = u\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.8238805970149253\n",
            "\n",
            "Naive - Bayes   ----->   0.6716417910447762\n",
            "\n",
            "LogisticRegression   ----->   0.7970149253731343\n",
            "\n",
            "Random Forest Classifier   ----->   0.7880597014925373\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzcoabkTphWX",
        "colab_type": "text"
      },
      "source": [
        "#### ***Models perform the best when we use features ( Title + Body + url )***\n",
        "- Now we will explore deep-learning approaches for text classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M6GFFMCtRTW",
        "colab_type": "text"
      },
      "source": [
        "# ***2. Deep Learning Models***\n",
        "- We will be considering the following deep learning methods for text classification\n",
        " 1. CNN + Word Embeddings\n",
        " 2. LSTMs\n",
        " 3. Stacked LSTMS\n",
        " 4. Bidirectional LSTMs\n",
        " 5. Hybrid Model --> (CNN + LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_CAC5jEWO04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating embeddings dictionary\n",
        "embeddings_dict = {}\n",
        "with open('/content/drive/My Drive/glove.6B.50d.txt') as f: \n",
        "    for line in f:\n",
        "        word = line.split()[0]\n",
        "        embeddings_dict[word] = np.array(line.split()[1:],dtype = 'float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65fmbv8B0sEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_for_model(max_len , vocab_size = 'None'):\n",
        "      (X_train, X_test, y_train, y_test) = prepare_data(['title','body','url','flair'])\n",
        "      tk = Tokenizer()\n",
        "      tk.fit_on_texts(X_train)\n",
        "      if vocab_size != 'None':\n",
        "        tk.word_index = {w:i for w,i in tk.word_index.items() if i <= vocab_size}\n",
        "      else:\n",
        "        vocab_size = len(tk.word_index)\n",
        "\n",
        "      # Train Data - X\n",
        "      encoded_train = tk.texts_to_sequences(X_train)\n",
        "      padded_train = pad_sequences(encoded_train,maxlen = max_len, padding = 'post' , truncating = 'post')\n",
        "\n",
        "      # Test Data  - X\n",
        "      encoded_test = tk.texts_to_sequences(X_test)\n",
        "      padded_test = pad_sequences(encoded_test,maxlen = max_len,padding = 'post', truncating = 'post')\n",
        "\n",
        "      # Train Data - Y\n",
        "      one_hot = OneHotEncoder()\n",
        "      y_train = y_train.reshape((-1,1))\n",
        "      y_train = one_hot.fit_transform(y_train).toarray()\n",
        "\n",
        "      # Create embeddings matrix\n",
        "      embeddings_matrix = np.zeros((vocab_size+1,50)) \n",
        "      for word,index in tk.word_index.items():\n",
        "        if embeddings_dict.get(word) is not None:\n",
        "          embeddings_matrix[index] = embeddings_dict[word]\n",
        "     \n",
        "      return (padded_train, y_train, padded_test, y_test, embeddings_matrix, vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPaLzvdJmSe0",
        "colab_type": "text"
      },
      "source": [
        "## ***2.1 CNN + Word Embeddings***\n",
        "- (**Yoav Goldberg**, in his primer on deep learning for nlp, 2015) , comments that CNNs are effective at Text Classification because of their capability to extract important features  (like tokens or sequence of tokens) regardless of their position in text.\n",
        "\n",
        "- He also comments, in text classification, the main idea is to learn words or a group of words that are good indicators of a topic, we do not necessarily care where they might appear in a document. Convolutions and pooling layers allow model to learn such local indicators invariant to their position.\n",
        "---\n",
        "1. For each training example, we will extract word embedding vectors (Glove vectors).\n",
        "2. we will feed this as input to CNN model, Our hope is that CNN model will learn some useful filters. A useful filter might be one , that has a similar embedding vector to the vector of a distinguishing word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wdm5y11JTw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix, vocab_len) = create_data_for_model(max_len = 300)\n",
        "def cnn_model(max_len, vocab_size):  \n",
        "  pooling_layers = []\n",
        "  inp = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim= vocab_size + 1,output_dim=50,weights = [embeddings_matrix],trainable = True)(inp)\n",
        "  dp = Dropout(0.3)(embedding_layer)\n",
        "\n",
        "  x1 = Conv1D(filters=36,kernel_size=1,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len,stride = 1)(x1))\n",
        "\n",
        "  x2 = Conv1D(filters=36,kernel_size=2,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 1,stride = 1)(x2))\n",
        "\n",
        "  x3 = Conv1D(filters=36,kernel_size=3,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 2,stride = 1)(x3))\n",
        "\n",
        "  x4 = Conv1D(filters=36,kernel_size=4,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 3,stride = 1)(x4))\n",
        "\n",
        "  x5 = Conv1D(filters=36,kernel_size=5,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 4,stride = 1)(x5))\n",
        "\n",
        "  z = Concatenate(axis = 1)(pooling_layers)\n",
        "  z = Flatten()(z)\n",
        "  #z = Dropout(0.3)(z)\n",
        "\n",
        "  \n",
        "  y = Dense(16,activation = 'relu')(z)\n",
        "\n",
        "  out = Dense(11,activation = 'softmax')(y)\n",
        "\n",
        "\n",
        "  model = Model(inputs = inp,outputs = out)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjKJ_YArwqHV",
        "colab_type": "code",
        "outputId": "0d7d4c36-7179-48ff-f68a-99da099e83db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = cnn_model(max_len = 300,vocab_size = vocab_len)\n",
        "history = model.fit(padded_train,y_train, epochs = 200, batch_size = 32, validation_split = 0.15)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=300, strides=1)`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=299, strides=1)`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=298, strides=1)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=297, strides=1)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=296, strides=1)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 300, 50)      1018350     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 300, 50)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 300, 36)      1836        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 299, 36)      3636        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 298, 36)      5436        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 297, 36)      7236        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 296, 36)      9036        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 1, 36)        0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 1, 36)        0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 1, 36)        0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, 1, 36)        0           conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling1D) (None, 1, 36)        0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 5, 36)        0           max_pooling1d_6[0][0]            \n",
            "                                                                 max_pooling1d_7[0][0]            \n",
            "                                                                 max_pooling1d_8[0][0]            \n",
            "                                                                 max_pooling1d_9[0][0]            \n",
            "                                                                 max_pooling1d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 180)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 16)           2896        flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 11)           187         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,048,613\n",
            "Trainable params: 1,048,613\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/200\n",
            "1608/1608 [==============================] - 1s 496us/step - loss: 2.4504 - accuracy: 0.1312 - val_loss: 2.2719 - val_accuracy: 0.1373\n",
            "Epoch 2/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 2.2115 - accuracy: 0.2264 - val_loss: 2.1212 - val_accuracy: 0.2324\n",
            "Epoch 3/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 2.0288 - accuracy: 0.3072 - val_loss: 1.8879 - val_accuracy: 0.3556\n",
            "Epoch 4/200\n",
            "1608/1608 [==============================] - 0s 284us/step - loss: 1.7626 - accuracy: 0.4042 - val_loss: 1.5765 - val_accuracy: 0.4085\n",
            "Epoch 5/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 1.4507 - accuracy: 0.5398 - val_loss: 1.2308 - val_accuracy: 0.5845\n",
            "Epoch 6/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 1.1692 - accuracy: 0.6200 - val_loss: 1.0139 - val_accuracy: 0.7007\n",
            "Epoch 7/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.9829 - accuracy: 0.6859 - val_loss: 0.9015 - val_accuracy: 0.7007\n",
            "Epoch 8/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.8536 - accuracy: 0.7407 - val_loss: 0.8116 - val_accuracy: 0.7394\n",
            "Epoch 9/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.7180 - accuracy: 0.7867 - val_loss: 0.7820 - val_accuracy: 0.7394\n",
            "Epoch 10/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.6354 - accuracy: 0.7879 - val_loss: 0.7453 - val_accuracy: 0.7570\n",
            "Epoch 11/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.5526 - accuracy: 0.8402 - val_loss: 0.7470 - val_accuracy: 0.7676\n",
            "Epoch 12/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.5187 - accuracy: 0.8439 - val_loss: 0.7116 - val_accuracy: 0.7746\n",
            "Epoch 13/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.4235 - accuracy: 0.8818 - val_loss: 0.7137 - val_accuracy: 0.7641\n",
            "Epoch 14/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.3903 - accuracy: 0.8887 - val_loss: 0.6642 - val_accuracy: 0.7993\n",
            "Epoch 15/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.3415 - accuracy: 0.9104 - val_loss: 0.6995 - val_accuracy: 0.7852\n",
            "Epoch 16/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.3060 - accuracy: 0.9080 - val_loss: 0.6735 - val_accuracy: 0.7958\n",
            "Epoch 17/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.2734 - accuracy: 0.9223 - val_loss: 0.6791 - val_accuracy: 0.7958\n",
            "Epoch 18/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.2257 - accuracy: 0.9434 - val_loss: 0.6850 - val_accuracy: 0.7993\n",
            "Epoch 19/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.2314 - accuracy: 0.9291 - val_loss: 0.6901 - val_accuracy: 0.8063\n",
            "Epoch 20/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.2146 - accuracy: 0.9490 - val_loss: 0.7118 - val_accuracy: 0.8134\n",
            "Epoch 21/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.2146 - accuracy: 0.9409 - val_loss: 0.6873 - val_accuracy: 0.8134\n",
            "Epoch 22/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.1654 - accuracy: 0.9627 - val_loss: 0.7066 - val_accuracy: 0.8204\n",
            "Epoch 23/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.1785 - accuracy: 0.9515 - val_loss: 0.7309 - val_accuracy: 0.8028\n",
            "Epoch 24/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.1412 - accuracy: 0.9664 - val_loss: 0.7230 - val_accuracy: 0.8204\n",
            "Epoch 25/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.1300 - accuracy: 0.9708 - val_loss: 0.7234 - val_accuracy: 0.8063\n",
            "Epoch 26/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.1222 - accuracy: 0.9701 - val_loss: 0.7338 - val_accuracy: 0.8028\n",
            "Epoch 27/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.1177 - accuracy: 0.9745 - val_loss: 0.7407 - val_accuracy: 0.8099\n",
            "Epoch 28/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.1092 - accuracy: 0.9720 - val_loss: 0.7630 - val_accuracy: 0.8063\n",
            "Epoch 29/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.1070 - accuracy: 0.9726 - val_loss: 0.7596 - val_accuracy: 0.8028\n",
            "Epoch 30/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.1161 - accuracy: 0.9714 - val_loss: 0.7709 - val_accuracy: 0.8134\n",
            "Epoch 31/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.1018 - accuracy: 0.9739 - val_loss: 0.8107 - val_accuracy: 0.8063\n",
            "Epoch 32/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.1004 - accuracy: 0.9714 - val_loss: 0.8131 - val_accuracy: 0.7958\n",
            "Epoch 33/200\n",
            "1608/1608 [==============================] - 0s 282us/step - loss: 0.1189 - accuracy: 0.9689 - val_loss: 0.7906 - val_accuracy: 0.8204\n",
            "Epoch 34/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.0925 - accuracy: 0.9745 - val_loss: 0.8257 - val_accuracy: 0.7923\n",
            "Epoch 35/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.0896 - accuracy: 0.9757 - val_loss: 0.7920 - val_accuracy: 0.8275\n",
            "Epoch 36/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.1057 - accuracy: 0.9701 - val_loss: 0.8070 - val_accuracy: 0.8028\n",
            "Epoch 37/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0753 - accuracy: 0.9789 - val_loss: 0.8196 - val_accuracy: 0.8169\n",
            "Epoch 38/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0873 - accuracy: 0.9764 - val_loss: 0.8333 - val_accuracy: 0.8169\n",
            "Epoch 39/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.0776 - accuracy: 0.9764 - val_loss: 0.8252 - val_accuracy: 0.8099\n",
            "Epoch 40/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0658 - accuracy: 0.9764 - val_loss: 0.8265 - val_accuracy: 0.8169\n",
            "Epoch 41/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0760 - accuracy: 0.9739 - val_loss: 0.8673 - val_accuracy: 0.8028\n",
            "Epoch 42/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0921 - accuracy: 0.9726 - val_loss: 0.8714 - val_accuracy: 0.8204\n",
            "Epoch 43/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0882 - accuracy: 0.9776 - val_loss: 0.8559 - val_accuracy: 0.8063\n",
            "Epoch 44/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0687 - accuracy: 0.9795 - val_loss: 0.8680 - val_accuracy: 0.8239\n",
            "Epoch 45/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.0542 - accuracy: 0.9795 - val_loss: 0.8990 - val_accuracy: 0.8169\n",
            "Epoch 46/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0758 - accuracy: 0.9801 - val_loss: 0.9136 - val_accuracy: 0.8134\n",
            "Epoch 47/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.0699 - accuracy: 0.9757 - val_loss: 0.8924 - val_accuracy: 0.7923\n",
            "Epoch 48/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0676 - accuracy: 0.9795 - val_loss: 0.9249 - val_accuracy: 0.7958\n",
            "Epoch 49/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0764 - accuracy: 0.9739 - val_loss: 0.9304 - val_accuracy: 0.8028\n",
            "Epoch 50/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0597 - accuracy: 0.9832 - val_loss: 0.9184 - val_accuracy: 0.7958\n",
            "Epoch 51/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0637 - accuracy: 0.9782 - val_loss: 0.8764 - val_accuracy: 0.8204\n",
            "Epoch 52/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0739 - accuracy: 0.9770 - val_loss: 0.8664 - val_accuracy: 0.8204\n",
            "Epoch 53/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0654 - accuracy: 0.9764 - val_loss: 0.8603 - val_accuracy: 0.8275\n",
            "Epoch 54/200\n",
            "1608/1608 [==============================] - 0s 282us/step - loss: 0.0622 - accuracy: 0.9782 - val_loss: 0.8834 - val_accuracy: 0.8169\n",
            "Epoch 55/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0670 - accuracy: 0.9807 - val_loss: 0.9581 - val_accuracy: 0.8063\n",
            "Epoch 56/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.0709 - accuracy: 0.9751 - val_loss: 0.9141 - val_accuracy: 0.8275\n",
            "Epoch 57/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0724 - accuracy: 0.9764 - val_loss: 0.9352 - val_accuracy: 0.7993\n",
            "Epoch 58/200\n",
            "1608/1608 [==============================] - 0s 288us/step - loss: 0.0681 - accuracy: 0.9795 - val_loss: 0.8351 - val_accuracy: 0.8275\n",
            "Epoch 59/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0567 - accuracy: 0.9789 - val_loss: 0.9131 - val_accuracy: 0.8028\n",
            "Epoch 60/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0693 - accuracy: 0.9789 - val_loss: 0.8875 - val_accuracy: 0.8310\n",
            "Epoch 61/200\n",
            "1608/1608 [==============================] - 0s 282us/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 0.9024 - val_accuracy: 0.8204\n",
            "Epoch 62/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0712 - accuracy: 0.9757 - val_loss: 0.9355 - val_accuracy: 0.8134\n",
            "Epoch 63/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0689 - accuracy: 0.9751 - val_loss: 0.8557 - val_accuracy: 0.8169\n",
            "Epoch 64/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0716 - accuracy: 0.9795 - val_loss: 0.9684 - val_accuracy: 0.8063\n",
            "Epoch 65/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0560 - accuracy: 0.9795 - val_loss: 0.9848 - val_accuracy: 0.8028\n",
            "Epoch 66/200\n",
            "1608/1608 [==============================] - 0s 270us/step - loss: 0.0461 - accuracy: 0.9838 - val_loss: 0.9023 - val_accuracy: 0.8345\n",
            "Epoch 67/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.0789 - accuracy: 0.9720 - val_loss: 0.9291 - val_accuracy: 0.8345\n",
            "Epoch 68/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0767 - accuracy: 0.9770 - val_loss: 0.9035 - val_accuracy: 0.8275\n",
            "Epoch 69/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0672 - accuracy: 0.9757 - val_loss: 0.8976 - val_accuracy: 0.8204\n",
            "Epoch 70/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0609 - accuracy: 0.9776 - val_loss: 0.8963 - val_accuracy: 0.8275\n",
            "Epoch 71/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0495 - accuracy: 0.9820 - val_loss: 0.9495 - val_accuracy: 0.8134\n",
            "Epoch 72/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0718 - accuracy: 0.9795 - val_loss: 0.9013 - val_accuracy: 0.8345\n",
            "Epoch 73/200\n",
            "1608/1608 [==============================] - 0s 290us/step - loss: 0.0577 - accuracy: 0.9795 - val_loss: 0.9164 - val_accuracy: 0.8099\n",
            "Epoch 74/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0557 - accuracy: 0.9807 - val_loss: 0.9400 - val_accuracy: 0.8169\n",
            "Epoch 75/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0602 - accuracy: 0.9826 - val_loss: 0.9057 - val_accuracy: 0.8134\n",
            "Epoch 76/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.0596 - accuracy: 0.9795 - val_loss: 0.9805 - val_accuracy: 0.7958\n",
            "Epoch 77/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0886 - accuracy: 0.9745 - val_loss: 0.9254 - val_accuracy: 0.8345\n",
            "Epoch 78/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0489 - accuracy: 0.9851 - val_loss: 0.9132 - val_accuracy: 0.8204\n",
            "Epoch 79/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.0559 - accuracy: 0.9789 - val_loss: 0.8862 - val_accuracy: 0.8345\n",
            "Epoch 80/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0690 - accuracy: 0.9757 - val_loss: 0.9238 - val_accuracy: 0.8239\n",
            "Epoch 81/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.0549 - accuracy: 0.9776 - val_loss: 0.8999 - val_accuracy: 0.8310\n",
            "Epoch 82/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0524 - accuracy: 0.9826 - val_loss: 0.9797 - val_accuracy: 0.8204\n",
            "Epoch 83/200\n",
            "1608/1608 [==============================] - 0s 290us/step - loss: 0.0581 - accuracy: 0.9757 - val_loss: 0.9572 - val_accuracy: 0.8099\n",
            "Epoch 84/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0631 - accuracy: 0.9807 - val_loss: 0.9610 - val_accuracy: 0.8204\n",
            "Epoch 85/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0640 - accuracy: 0.9776 - val_loss: 0.9484 - val_accuracy: 0.8063\n",
            "Epoch 86/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0632 - accuracy: 0.9820 - val_loss: 0.9191 - val_accuracy: 0.8063\n",
            "Epoch 87/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.9670 - val_accuracy: 0.8134\n",
            "Epoch 88/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0466 - accuracy: 0.9820 - val_loss: 0.9037 - val_accuracy: 0.8239\n",
            "Epoch 89/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0468 - accuracy: 0.9807 - val_loss: 0.9663 - val_accuracy: 0.8028\n",
            "Epoch 90/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.0561 - accuracy: 0.9813 - val_loss: 0.9492 - val_accuracy: 0.8099\n",
            "Epoch 91/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0845 - accuracy: 0.9770 - val_loss: 0.9565 - val_accuracy: 0.8028\n",
            "Epoch 92/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.0486 - accuracy: 0.9813 - val_loss: 0.9172 - val_accuracy: 0.8345\n",
            "Epoch 93/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0551 - accuracy: 0.9789 - val_loss: 1.0463 - val_accuracy: 0.8028\n",
            "Epoch 94/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0529 - accuracy: 0.9776 - val_loss: 0.9307 - val_accuracy: 0.8204\n",
            "Epoch 95/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0572 - accuracy: 0.9789 - val_loss: 0.9567 - val_accuracy: 0.8028\n",
            "Epoch 96/200\n",
            "1608/1608 [==============================] - 0s 287us/step - loss: 0.0495 - accuracy: 0.9776 - val_loss: 0.9886 - val_accuracy: 0.8239\n",
            "Epoch 97/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0501 - accuracy: 0.9813 - val_loss: 0.8890 - val_accuracy: 0.8345\n",
            "Epoch 98/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0616 - accuracy: 0.9789 - val_loss: 0.9195 - val_accuracy: 0.8099\n",
            "Epoch 99/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.0533 - accuracy: 0.9807 - val_loss: 0.9658 - val_accuracy: 0.8204\n",
            "Epoch 100/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0539 - accuracy: 0.9807 - val_loss: 1.1160 - val_accuracy: 0.7923\n",
            "Epoch 101/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0687 - accuracy: 0.9770 - val_loss: 1.0012 - val_accuracy: 0.8204\n",
            "Epoch 102/200\n",
            "1608/1608 [==============================] - 0s 270us/step - loss: 0.0649 - accuracy: 0.9782 - val_loss: 0.9606 - val_accuracy: 0.8134\n",
            "Epoch 103/200\n",
            "1608/1608 [==============================] - 0s 282us/step - loss: 0.0576 - accuracy: 0.9776 - val_loss: 0.9563 - val_accuracy: 0.8275\n",
            "Epoch 104/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.0573 - accuracy: 0.9813 - val_loss: 1.0736 - val_accuracy: 0.8063\n",
            "Epoch 105/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0639 - accuracy: 0.9776 - val_loss: 0.9682 - val_accuracy: 0.8134\n",
            "Epoch 106/200\n",
            "1608/1608 [==============================] - 0s 292us/step - loss: 0.0555 - accuracy: 0.9795 - val_loss: 0.9503 - val_accuracy: 0.8134\n",
            "Epoch 107/200\n",
            "1608/1608 [==============================] - 0s 302us/step - loss: 0.0642 - accuracy: 0.9820 - val_loss: 1.0286 - val_accuracy: 0.8099\n",
            "Epoch 108/200\n",
            "1608/1608 [==============================] - 0s 289us/step - loss: 0.0619 - accuracy: 0.9764 - val_loss: 1.0103 - val_accuracy: 0.8134\n",
            "Epoch 109/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0532 - accuracy: 0.9776 - val_loss: 1.1085 - val_accuracy: 0.7852\n",
            "Epoch 110/200\n",
            "1608/1608 [==============================] - 0s 287us/step - loss: 0.0438 - accuracy: 0.9838 - val_loss: 0.9867 - val_accuracy: 0.8099\n",
            "Epoch 111/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0541 - accuracy: 0.9813 - val_loss: 1.0201 - val_accuracy: 0.8099\n",
            "Epoch 112/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0550 - accuracy: 0.9770 - val_loss: 0.9901 - val_accuracy: 0.8310\n",
            "Epoch 113/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0453 - accuracy: 0.9851 - val_loss: 1.0409 - val_accuracy: 0.7887\n",
            "Epoch 114/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.0576 - accuracy: 0.9826 - val_loss: 0.9780 - val_accuracy: 0.8275\n",
            "Epoch 115/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0533 - accuracy: 0.9813 - val_loss: 0.9969 - val_accuracy: 0.8063\n",
            "Epoch 116/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0619 - accuracy: 0.9770 - val_loss: 1.0487 - val_accuracy: 0.8063\n",
            "Epoch 117/200\n",
            "1608/1608 [==============================] - 0s 284us/step - loss: 0.0596 - accuracy: 0.9789 - val_loss: 0.9736 - val_accuracy: 0.8063\n",
            "Epoch 118/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.0463 - accuracy: 0.9795 - val_loss: 1.0421 - val_accuracy: 0.8063\n",
            "Epoch 119/200\n",
            "1608/1608 [==============================] - 0s 282us/step - loss: 0.0488 - accuracy: 0.9795 - val_loss: 0.9901 - val_accuracy: 0.8204\n",
            "Epoch 120/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0622 - accuracy: 0.9776 - val_loss: 1.0786 - val_accuracy: 0.8063\n",
            "Epoch 121/200\n",
            "1608/1608 [==============================] - 0s 270us/step - loss: 0.0523 - accuracy: 0.9776 - val_loss: 0.9894 - val_accuracy: 0.8099\n",
            "Epoch 122/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0391 - accuracy: 0.9820 - val_loss: 0.9512 - val_accuracy: 0.8204\n",
            "Epoch 123/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0466 - accuracy: 0.9813 - val_loss: 1.0665 - val_accuracy: 0.7993\n",
            "Epoch 124/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0467 - accuracy: 0.9845 - val_loss: 1.0418 - val_accuracy: 0.8028\n",
            "Epoch 125/200\n",
            "1608/1608 [==============================] - 0s 270us/step - loss: 0.0540 - accuracy: 0.9795 - val_loss: 0.9710 - val_accuracy: 0.8169\n",
            "Epoch 126/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.0481 - accuracy: 0.9795 - val_loss: 1.0402 - val_accuracy: 0.8134\n",
            "Epoch 127/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0465 - accuracy: 0.9813 - val_loss: 0.9928 - val_accuracy: 0.8169\n",
            "Epoch 128/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0467 - accuracy: 0.9813 - val_loss: 1.0261 - val_accuracy: 0.8134\n",
            "Epoch 129/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0469 - accuracy: 0.9813 - val_loss: 1.0204 - val_accuracy: 0.8204\n",
            "Epoch 130/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0555 - accuracy: 0.9813 - val_loss: 1.0091 - val_accuracy: 0.8239\n",
            "Epoch 131/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0501 - accuracy: 0.9789 - val_loss: 1.0156 - val_accuracy: 0.8099\n",
            "Epoch 132/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0458 - accuracy: 0.9801 - val_loss: 1.0191 - val_accuracy: 0.8239\n",
            "Epoch 133/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0496 - accuracy: 0.9795 - val_loss: 1.0265 - val_accuracy: 0.8134\n",
            "Epoch 134/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0384 - accuracy: 0.9832 - val_loss: 0.9885 - val_accuracy: 0.8134\n",
            "Epoch 135/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0524 - accuracy: 0.9813 - val_loss: 1.0725 - val_accuracy: 0.8204\n",
            "Epoch 136/200\n",
            "1608/1608 [==============================] - 0s 282us/step - loss: 0.0508 - accuracy: 0.9795 - val_loss: 1.0080 - val_accuracy: 0.8345\n",
            "Epoch 137/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0533 - accuracy: 0.9826 - val_loss: 0.9857 - val_accuracy: 0.8099\n",
            "Epoch 138/200\n",
            "1608/1608 [==============================] - 0s 270us/step - loss: 0.0572 - accuracy: 0.9782 - val_loss: 1.1267 - val_accuracy: 0.7852\n",
            "Epoch 139/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0615 - accuracy: 0.9795 - val_loss: 1.0161 - val_accuracy: 0.8063\n",
            "Epoch 140/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.0498 - accuracy: 0.9789 - val_loss: 0.9682 - val_accuracy: 0.8169\n",
            "Epoch 141/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0409 - accuracy: 0.9807 - val_loss: 1.0190 - val_accuracy: 0.8204\n",
            "Epoch 142/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0479 - accuracy: 0.9807 - val_loss: 0.9920 - val_accuracy: 0.8169\n",
            "Epoch 143/200\n",
            "1608/1608 [==============================] - 0s 270us/step - loss: 0.0480 - accuracy: 0.9801 - val_loss: 1.0802 - val_accuracy: 0.7852\n",
            "Epoch 144/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0532 - accuracy: 0.9807 - val_loss: 1.0593 - val_accuracy: 0.8099\n",
            "Epoch 145/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0493 - accuracy: 0.9801 - val_loss: 0.9829 - val_accuracy: 0.8063\n",
            "Epoch 146/200\n",
            "1608/1608 [==============================] - 0s 268us/step - loss: 0.0556 - accuracy: 0.9782 - val_loss: 1.0536 - val_accuracy: 0.7958\n",
            "Epoch 147/200\n",
            "1608/1608 [==============================] - 0s 294us/step - loss: 0.0460 - accuracy: 0.9826 - val_loss: 0.9980 - val_accuracy: 0.8134\n",
            "Epoch 148/200\n",
            "1608/1608 [==============================] - 0s 286us/step - loss: 0.0567 - accuracy: 0.9764 - val_loss: 1.0224 - val_accuracy: 0.8169\n",
            "Epoch 149/200\n",
            "1608/1608 [==============================] - 0s 287us/step - loss: 0.0534 - accuracy: 0.9801 - val_loss: 0.9902 - val_accuracy: 0.8099\n",
            "Epoch 150/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.0433 - accuracy: 0.9851 - val_loss: 1.0470 - val_accuracy: 0.8099\n",
            "Epoch 151/200\n",
            "1608/1608 [==============================] - 0s 286us/step - loss: 0.0447 - accuracy: 0.9832 - val_loss: 1.0056 - val_accuracy: 0.8169\n",
            "Epoch 152/200\n",
            "1608/1608 [==============================] - 0s 286us/step - loss: 0.0443 - accuracy: 0.9795 - val_loss: 1.0015 - val_accuracy: 0.8204\n",
            "Epoch 153/200\n",
            "1608/1608 [==============================] - 0s 286us/step - loss: 0.0591 - accuracy: 0.9795 - val_loss: 0.9911 - val_accuracy: 0.8169\n",
            "Epoch 154/200\n",
            "1608/1608 [==============================] - 0s 304us/step - loss: 0.0564 - accuracy: 0.9764 - val_loss: 1.0367 - val_accuracy: 0.8169\n",
            "Epoch 155/200\n",
            "1608/1608 [==============================] - 0s 290us/step - loss: 0.0494 - accuracy: 0.9789 - val_loss: 0.9841 - val_accuracy: 0.8063\n",
            "Epoch 156/200\n",
            "1608/1608 [==============================] - 0s 296us/step - loss: 0.0441 - accuracy: 0.9770 - val_loss: 1.0078 - val_accuracy: 0.8169\n",
            "Epoch 157/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0402 - accuracy: 0.9820 - val_loss: 1.0242 - val_accuracy: 0.8310\n",
            "Epoch 158/200\n",
            "1608/1608 [==============================] - 0s 293us/step - loss: 0.0505 - accuracy: 0.9776 - val_loss: 0.9731 - val_accuracy: 0.8239\n",
            "Epoch 159/200\n",
            "1608/1608 [==============================] - 0s 278us/step - loss: 0.0524 - accuracy: 0.9813 - val_loss: 1.0455 - val_accuracy: 0.8028\n",
            "Epoch 160/200\n",
            "1608/1608 [==============================] - 0s 286us/step - loss: 0.0589 - accuracy: 0.9820 - val_loss: 1.0371 - val_accuracy: 0.7958\n",
            "Epoch 161/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.0427 - accuracy: 0.9832 - val_loss: 1.0203 - val_accuracy: 0.7993\n",
            "Epoch 162/200\n",
            "1608/1608 [==============================] - 0s 285us/step - loss: 0.0499 - accuracy: 0.9826 - val_loss: 0.9938 - val_accuracy: 0.8204\n",
            "Epoch 163/200\n",
            "1608/1608 [==============================] - 0s 283us/step - loss: 0.0507 - accuracy: 0.9739 - val_loss: 0.9900 - val_accuracy: 0.8134\n",
            "Epoch 164/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0446 - accuracy: 0.9789 - val_loss: 1.0523 - val_accuracy: 0.8134\n",
            "Epoch 165/200\n",
            "1608/1608 [==============================] - 0s 287us/step - loss: 0.0475 - accuracy: 0.9789 - val_loss: 0.9805 - val_accuracy: 0.8169\n",
            "Epoch 166/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0547 - accuracy: 0.9782 - val_loss: 1.0092 - val_accuracy: 0.8204\n",
            "Epoch 167/200\n",
            "1608/1608 [==============================] - 0s 295us/step - loss: 0.0621 - accuracy: 0.9813 - val_loss: 1.0546 - val_accuracy: 0.7923\n",
            "Epoch 168/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0430 - accuracy: 0.9795 - val_loss: 0.9846 - val_accuracy: 0.8169\n",
            "Epoch 169/200\n",
            "1608/1608 [==============================] - 0s 287us/step - loss: 0.0443 - accuracy: 0.9795 - val_loss: 0.9809 - val_accuracy: 0.8099\n",
            "Epoch 170/200\n",
            "1608/1608 [==============================] - 0s 269us/step - loss: 0.0403 - accuracy: 0.9813 - val_loss: 0.9465 - val_accuracy: 0.8380\n",
            "Epoch 171/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0376 - accuracy: 0.9826 - val_loss: 0.9989 - val_accuracy: 0.8134\n",
            "Epoch 172/200\n",
            "1608/1608 [==============================] - 0s 273us/step - loss: 0.0490 - accuracy: 0.9764 - val_loss: 1.0621 - val_accuracy: 0.8099\n",
            "Epoch 173/200\n",
            "1608/1608 [==============================] - 0s 266us/step - loss: 0.0517 - accuracy: 0.9782 - val_loss: 0.9485 - val_accuracy: 0.8204\n",
            "Epoch 174/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0416 - accuracy: 0.9776 - val_loss: 1.0249 - val_accuracy: 0.8239\n",
            "Epoch 175/200\n",
            "1608/1608 [==============================] - 0s 265us/step - loss: 0.0451 - accuracy: 0.9795 - val_loss: 1.0285 - val_accuracy: 0.8063\n",
            "Epoch 176/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0409 - accuracy: 0.9813 - val_loss: 1.0454 - val_accuracy: 0.8239\n",
            "Epoch 177/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0491 - accuracy: 0.9776 - val_loss: 1.0177 - val_accuracy: 0.8204\n",
            "Epoch 178/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0501 - accuracy: 0.9776 - val_loss: 0.9946 - val_accuracy: 0.8275\n",
            "Epoch 179/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0466 - accuracy: 0.9789 - val_loss: 0.9588 - val_accuracy: 0.8275\n",
            "Epoch 180/200\n",
            "1608/1608 [==============================] - 0s 269us/step - loss: 0.0461 - accuracy: 0.9807 - val_loss: 0.9924 - val_accuracy: 0.8239\n",
            "Epoch 181/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0418 - accuracy: 0.9807 - val_loss: 1.0247 - val_accuracy: 0.8239\n",
            "Epoch 182/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0401 - accuracy: 0.9820 - val_loss: 0.9708 - val_accuracy: 0.8239\n",
            "Epoch 183/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0488 - accuracy: 0.9801 - val_loss: 1.0455 - val_accuracy: 0.8169\n",
            "Epoch 184/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0493 - accuracy: 0.9795 - val_loss: 1.0338 - val_accuracy: 0.8099\n",
            "Epoch 185/200\n",
            "1608/1608 [==============================] - 0s 284us/step - loss: 0.0520 - accuracy: 0.9801 - val_loss: 1.0095 - val_accuracy: 0.7993\n",
            "Epoch 186/200\n",
            "1608/1608 [==============================] - 0s 272us/step - loss: 0.0478 - accuracy: 0.9776 - val_loss: 0.9869 - val_accuracy: 0.8169\n",
            "Epoch 187/200\n",
            "1608/1608 [==============================] - 0s 276us/step - loss: 0.0440 - accuracy: 0.9776 - val_loss: 0.9401 - val_accuracy: 0.8204\n",
            "Epoch 188/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0343 - accuracy: 0.9845 - val_loss: 1.1796 - val_accuracy: 0.8028\n",
            "Epoch 189/200\n",
            "1608/1608 [==============================] - 0s 268us/step - loss: 0.0448 - accuracy: 0.9801 - val_loss: 0.9829 - val_accuracy: 0.8169\n",
            "Epoch 190/200\n",
            "1608/1608 [==============================] - 0s 277us/step - loss: 0.0382 - accuracy: 0.9801 - val_loss: 1.0652 - val_accuracy: 0.8099\n",
            "Epoch 191/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0533 - accuracy: 0.9789 - val_loss: 1.0600 - val_accuracy: 0.7887\n",
            "Epoch 192/200\n",
            "1608/1608 [==============================] - 0s 281us/step - loss: 0.0507 - accuracy: 0.9776 - val_loss: 1.0200 - val_accuracy: 0.8028\n",
            "Epoch 193/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0448 - accuracy: 0.9789 - val_loss: 1.0039 - val_accuracy: 0.8169\n",
            "Epoch 194/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0379 - accuracy: 0.9782 - val_loss: 0.9773 - val_accuracy: 0.8169\n",
            "Epoch 195/200\n",
            "1608/1608 [==============================] - 0s 271us/step - loss: 0.0495 - accuracy: 0.9807 - val_loss: 1.0124 - val_accuracy: 0.8028\n",
            "Epoch 196/200\n",
            "1608/1608 [==============================] - 0s 274us/step - loss: 0.0483 - accuracy: 0.9795 - val_loss: 0.9901 - val_accuracy: 0.8239\n",
            "Epoch 197/200\n",
            "1608/1608 [==============================] - 0s 287us/step - loss: 0.0411 - accuracy: 0.9838 - val_loss: 1.0806 - val_accuracy: 0.7993\n",
            "Epoch 198/200\n",
            "1608/1608 [==============================] - 0s 280us/step - loss: 0.0416 - accuracy: 0.9795 - val_loss: 1.0263 - val_accuracy: 0.8063\n",
            "Epoch 199/200\n",
            "1608/1608 [==============================] - 0s 279us/step - loss: 0.0634 - accuracy: 0.9782 - val_loss: 1.0043 - val_accuracy: 0.8169\n",
            "Epoch 200/200\n",
            "1608/1608 [==============================] - 0s 275us/step - loss: 0.0436 - accuracy: 0.9795 - val_loss: 0.9727 - val_accuracy: 0.8063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdGGd_QqwYzp",
        "colab_type": "code",
        "outputId": "da18acbf-3aad-4ec5-c0ee-ce8e08182b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "predicted = model.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8328358208955224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzPeq5kyeY7E",
        "colab_type": "text"
      },
      "source": [
        "0.8388059701492537 - max_len = 300 vocab - 21000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke1v_BxZrFOL",
        "colab_type": "code",
        "outputId": "eefd5670-c2bf-46be-d3b3-d98c0097434c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd87a195240>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWO8verzPqS",
        "colab_type": "text"
      },
      "source": [
        "0.8119402985074626 - The best till now "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U50-6zDu4w4S",
        "colab_type": "text"
      },
      "source": [
        "# **2. LSTM**\n",
        "- The idea behind using LSTMs is to capture \n",
        "![hf](https://github.com/mananm98/image_captioning/blob/master/images/Unknown-2.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT2rqN5wu0aA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   LSTM slow to train larger max_len cannot say about vocab size no improvement from 9000 to 21000\n",
        "*   will try on max_len = 300\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTr-pcGt66jf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "53ebcb72-5010-4403-f6f9-61e75e2fc206"
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix,vocab_len) = create_data_for_model(vocab_size = 12000, max_len = 35)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e16af2d4a99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mpadded_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_matrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'create_data_for_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0vrDEyi4qIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Model\n",
        "def model_lstm(max_len,vocab_size):\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=True,weights = [embeddings_matrix],trainable = False)(inp1)\n",
        "  drop_out1 = Dropout(0.3)(embedding_layer)\n",
        "  lstm_layer1 = LSTM(32,return_sequences = False)(drop_out1)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  model_lstm = Model(inputs = inp1,outputs = out1)\n",
        "  model_lstm.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model_lstm.summary()\n",
        "  return model_lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boQBFLRV4p8q",
        "colab_type": "code",
        "outputId": "7ae9c291-baba-4afc-8f60-a7348244363a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = model_lstm(35,12000)\n",
        "hist = model.fit(padded_train,y_train,epochs = 30, batch_size=16,validation_split = 0.15)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_15 (InputLayer)        (None, 35)                0         \n",
            "_________________________________________________________________\n",
            "embedding_15 (Embedding)     (None, 35, 50)            600050    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 35, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 611,037\n",
            "Trainable params: 10,987\n",
            "Non-trainable params: 600,050\n",
            "_________________________________________________________________\n",
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/30\n",
            "1608/1608 [==============================] - 14s 9ms/step - loss: 2.3268 - accuracy: 0.1810 - val_loss: 2.1628 - val_accuracy: 0.2993\n",
            "Epoch 2/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 2.0658 - accuracy: 0.3184 - val_loss: 1.8635 - val_accuracy: 0.3627\n",
            "Epoch 3/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 1.8045 - accuracy: 0.4111 - val_loss: 1.6253 - val_accuracy: 0.4648\n",
            "Epoch 4/30\n",
            "1608/1608 [==============================] - 14s 9ms/step - loss: 1.6122 - accuracy: 0.4863 - val_loss: 1.5077 - val_accuracy: 0.5035\n",
            "Epoch 5/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 1.4848 - accuracy: 0.5236 - val_loss: 1.4056 - val_accuracy: 0.5669\n",
            "Epoch 6/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 1.3559 - accuracy: 0.5697 - val_loss: 1.3037 - val_accuracy: 0.5775\n",
            "Epoch 7/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 1.2740 - accuracy: 0.5964 - val_loss: 1.2621 - val_accuracy: 0.5915\n",
            "Epoch 8/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 1.2183 - accuracy: 0.6219 - val_loss: 1.2178 - val_accuracy: 0.6162\n",
            "Epoch 9/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 1.1614 - accuracy: 0.6443 - val_loss: 1.2309 - val_accuracy: 0.6197\n",
            "Epoch 10/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 1.1215 - accuracy: 0.6468 - val_loss: 1.2103 - val_accuracy: 0.6197\n",
            "Epoch 11/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 1.0537 - accuracy: 0.6660 - val_loss: 1.1468 - val_accuracy: 0.6338\n",
            "Epoch 12/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 1.0103 - accuracy: 0.6878 - val_loss: 1.1210 - val_accuracy: 0.6514\n",
            "Epoch 13/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.9817 - accuracy: 0.6847 - val_loss: 1.1441 - val_accuracy: 0.6408\n",
            "Epoch 14/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 0.9497 - accuracy: 0.7015 - val_loss: 1.1259 - val_accuracy: 0.6373\n",
            "Epoch 15/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.9225 - accuracy: 0.7077 - val_loss: 1.0958 - val_accuracy: 0.6479\n",
            "Epoch 16/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.8709 - accuracy: 0.7289 - val_loss: 1.1023 - val_accuracy: 0.6549\n",
            "Epoch 17/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.8647 - accuracy: 0.7257 - val_loss: 1.0732 - val_accuracy: 0.6620\n",
            "Epoch 18/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 0.8333 - accuracy: 0.7295 - val_loss: 1.0867 - val_accuracy: 0.6655\n",
            "Epoch 19/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.8133 - accuracy: 0.7394 - val_loss: 1.0800 - val_accuracy: 0.6620\n",
            "Epoch 20/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.8305 - accuracy: 0.7208 - val_loss: 1.0535 - val_accuracy: 0.6585\n",
            "Epoch 21/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.7950 - accuracy: 0.7425 - val_loss: 1.0671 - val_accuracy: 0.6585\n",
            "Epoch 22/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.7606 - accuracy: 0.7537 - val_loss: 1.0853 - val_accuracy: 0.6725\n",
            "Epoch 23/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.7576 - accuracy: 0.7531 - val_loss: 1.1061 - val_accuracy: 0.6761\n",
            "Epoch 24/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.7275 - accuracy: 0.7581 - val_loss: 1.1232 - val_accuracy: 0.6514\n",
            "Epoch 25/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.7054 - accuracy: 0.7693 - val_loss: 1.0795 - val_accuracy: 0.6408\n",
            "Epoch 26/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.6972 - accuracy: 0.7767 - val_loss: 1.1081 - val_accuracy: 0.6655\n",
            "Epoch 27/30\n",
            "1608/1608 [==============================] - 13s 8ms/step - loss: 0.6968 - accuracy: 0.7718 - val_loss: 1.0800 - val_accuracy: 0.6549\n",
            "Epoch 28/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 0.6540 - accuracy: 0.7836 - val_loss: 1.0858 - val_accuracy: 0.6549\n",
            "Epoch 29/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 0.6662 - accuracy: 0.7724 - val_loss: 1.1245 - val_accuracy: 0.6549\n",
            "Epoch 30/30\n",
            "1608/1608 [==============================] - 14s 8ms/step - loss: 0.6309 - accuracy: 0.7929 - val_loss: 1.1192 - val_accuracy: 0.6549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPgBSz8K5zbF",
        "colab_type": "code",
        "outputId": "41aa0844-b793-4131-b82c-88b8947f2785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = model.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6805970149253732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udgq9WsIvb_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0933b1d7-5288-4bd6-a251-a1f55f936335"
      },
      "source": [
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fca8e8e0e48>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA8cP0N-HvMI",
        "colab_type": "text"
      },
      "source": [
        "## **3. Stacked LSTMs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65OE7BGHwhmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix) = create_data_for_model(vocab_size = 12000, max_len = 35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3wjtHAD8NZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stackced_lstms(max_len,vocab_size):\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=True,weights = [embeddings_matrix],trainable = False)(inp1)\n",
        "  drop_out1 = Dropout(0.3)(embedding_layer)\n",
        "  lstm_layer1 = LSTM(64,return_sequences = True)(drop_out1)\n",
        "  lstm_layer2 = LSTM(64,return_sequences = False)(lstm_layer1)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer2)\n",
        "\n",
        "  model_lstm_stacked = Model(inputs = inp1,outputs = out1)\n",
        "  model_lstm_stacked.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model_lstm_stacked.summary()\n",
        "  return model_lstm_stacked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwW-iJ3GIbG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "866479ff-02b4-4298-cd57-340fb9dca0e4"
      },
      "source": [
        "model_lstm_stacked = stacked_lstms(35,12000)\n",
        "hist = model_lstm_stacked.fit(padded_train,y_train,epochs = 30, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        (None, 35)                0         \n",
            "_________________________________________________________________\n",
            "embedding_16 (Embedding)     (None, 35, 50)            600050    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 35, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 35, 64)            29440     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 11)                715       \n",
            "=================================================================\n",
            "Total params: 663,229\n",
            "Trainable params: 63,179\n",
            "Non-trainable params: 600,050\n",
            "_________________________________________________________________\n",
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/30\n",
            "1608/1608 [==============================] - 51s 31ms/step - loss: 1.9759 - accuracy: 0.3458 - val_loss: 1.5667 - val_accuracy: 0.4894\n",
            "Epoch 2/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 1.5553 - accuracy: 0.5019 - val_loss: 1.4297 - val_accuracy: 0.5352\n",
            "Epoch 3/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 1.3830 - accuracy: 0.5659 - val_loss: 1.3979 - val_accuracy: 0.5563\n",
            "Epoch 4/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 1.2557 - accuracy: 0.6051 - val_loss: 1.2643 - val_accuracy: 0.5915\n",
            "Epoch 5/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 1.1591 - accuracy: 0.6325 - val_loss: 1.2112 - val_accuracy: 0.6408\n",
            "Epoch 6/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 1.0742 - accuracy: 0.6573 - val_loss: 1.1577 - val_accuracy: 0.6268\n",
            "Epoch 7/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 1.0147 - accuracy: 0.6791 - val_loss: 1.0945 - val_accuracy: 0.6620\n",
            "Epoch 8/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.9507 - accuracy: 0.6922 - val_loss: 1.1762 - val_accuracy: 0.6444\n",
            "Epoch 9/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.9108 - accuracy: 0.7021 - val_loss: 1.1510 - val_accuracy: 0.6373\n",
            "Epoch 10/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.8640 - accuracy: 0.7177 - val_loss: 1.0530 - val_accuracy: 0.6761\n",
            "Epoch 11/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.7920 - accuracy: 0.7400 - val_loss: 1.0575 - val_accuracy: 0.6796\n",
            "Epoch 12/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.7568 - accuracy: 0.7581 - val_loss: 1.0467 - val_accuracy: 0.6831\n",
            "Epoch 13/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.7676 - accuracy: 0.7488 - val_loss: 1.1153 - val_accuracy: 0.6972\n",
            "Epoch 14/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.7161 - accuracy: 0.7612 - val_loss: 1.1419 - val_accuracy: 0.6972\n",
            "Epoch 15/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.6751 - accuracy: 0.7836 - val_loss: 1.0374 - val_accuracy: 0.6866\n",
            "Epoch 16/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.6604 - accuracy: 0.7904 - val_loss: 1.0398 - val_accuracy: 0.7077\n",
            "Epoch 17/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.6286 - accuracy: 0.7929 - val_loss: 1.0707 - val_accuracy: 0.7042\n",
            "Epoch 18/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.5852 - accuracy: 0.8103 - val_loss: 1.0879 - val_accuracy: 0.7148\n",
            "Epoch 19/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.5754 - accuracy: 0.8141 - val_loss: 1.1118 - val_accuracy: 0.6831\n",
            "Epoch 20/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.5854 - accuracy: 0.8085 - val_loss: 1.1074 - val_accuracy: 0.6831\n",
            "Epoch 21/30\n",
            "1608/1608 [==============================] - 48s 30ms/step - loss: 0.5230 - accuracy: 0.8277 - val_loss: 1.0768 - val_accuracy: 0.7148\n",
            "Epoch 22/30\n",
            "1608/1608 [==============================] - 48s 30ms/step - loss: 0.5018 - accuracy: 0.8259 - val_loss: 1.1405 - val_accuracy: 0.6937\n",
            "Epoch 23/30\n",
            "1608/1608 [==============================] - 48s 30ms/step - loss: 0.4676 - accuracy: 0.8396 - val_loss: 1.0970 - val_accuracy: 0.7254\n",
            "Epoch 24/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.4783 - accuracy: 0.8420 - val_loss: 1.1467 - val_accuracy: 0.6937\n",
            "Epoch 25/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.4628 - accuracy: 0.8451 - val_loss: 1.2043 - val_accuracy: 0.7007\n",
            "Epoch 26/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.4545 - accuracy: 0.8520 - val_loss: 1.2114 - val_accuracy: 0.6690\n",
            "Epoch 27/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.4656 - accuracy: 0.8377 - val_loss: 1.1975 - val_accuracy: 0.7042\n",
            "Epoch 28/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.3986 - accuracy: 0.8669 - val_loss: 1.2251 - val_accuracy: 0.7007\n",
            "Epoch 29/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.3824 - accuracy: 0.8731 - val_loss: 1.1684 - val_accuracy: 0.6972\n",
            "Epoch 30/30\n",
            "1608/1608 [==============================] - 49s 30ms/step - loss: 0.3629 - accuracy: 0.8800 - val_loss: 1.1705 - val_accuracy: 0.6866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1DcLp-UI4Cw",
        "colab_type": "code",
        "outputId": "ef5c7bde-07d7-4fde-b248-98c3977a6648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "predicted = model_lstm_stacked.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5757daf534c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lstm_stacked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_lstm_stacked' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3tNweWKJUlf",
        "colab_type": "text"
      },
      "source": [
        "# ***4. Bidirectional LSTM***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uyEkClHJT0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix) = create_data_for_model(vocab_size = 12000, max_len = 35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5u0P2GnJTyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_bilstm(max_len,vocab_size):\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=True,weights = [embeddings_matrix],trainable = False)(inp1)\n",
        "  drop_out1 = Dropout(0.3)(embedding_layer)\n",
        "  lstm_layer1 = Bidirectional(LSTM(64,return_sequences = False))(drop_out1)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  model_bi_lstm = Model(inputs = inp1,outputs = out1)\n",
        "  model_bi_lstm.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model_bi_lstm.summary()\n",
        "  return model_bi_lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p547SbAn8r64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6380c030-feac-456b-ce7a-a10ed3338ceb"
      },
      "source": [
        "model_bi_lstm = model_bilstm(35,12000)\n",
        "hist = model_bi_lstm.fit(padded_train,y_train,epochs = 30, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 35)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 35, 50)            600050    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 35, 50)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               58880     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 11)                1419      \n",
            "=================================================================\n",
            "Total params: 660,349\n",
            "Trainable params: 60,299\n",
            "Non-trainable params: 600,050\n",
            "_________________________________________________________________\n",
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 1.8967 - accuracy: 0.3787 - val_loss: 1.3738 - val_accuracy: 0.5915\n",
            "Epoch 2/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 1.3351 - accuracy: 0.5914 - val_loss: 1.1584 - val_accuracy: 0.6408\n",
            "Epoch 3/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 1.1250 - accuracy: 0.6374 - val_loss: 1.0693 - val_accuracy: 0.6549\n",
            "Epoch 4/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.9980 - accuracy: 0.6922 - val_loss: 1.0819 - val_accuracy: 0.6479\n",
            "Epoch 5/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.8849 - accuracy: 0.7102 - val_loss: 1.0028 - val_accuracy: 0.6901\n",
            "Epoch 6/30\n",
            "1608/1608 [==============================] - 51s 31ms/step - loss: 0.8132 - accuracy: 0.7394 - val_loss: 0.9888 - val_accuracy: 0.6937\n",
            "Epoch 7/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.7449 - accuracy: 0.7575 - val_loss: 0.9614 - val_accuracy: 0.7042\n",
            "Epoch 8/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.6585 - accuracy: 0.7836 - val_loss: 1.0137 - val_accuracy: 0.6761\n",
            "Epoch 9/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.6178 - accuracy: 0.7966 - val_loss: 1.0104 - val_accuracy: 0.7042\n",
            "Epoch 10/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.5578 - accuracy: 0.8159 - val_loss: 1.0835 - val_accuracy: 0.6866\n",
            "Epoch 11/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.5419 - accuracy: 0.8296 - val_loss: 1.0539 - val_accuracy: 0.6796\n",
            "Epoch 12/30\n",
            "1608/1608 [==============================] - 51s 31ms/step - loss: 0.4552 - accuracy: 0.8476 - val_loss: 1.0988 - val_accuracy: 0.6761\n",
            "Epoch 13/30\n",
            "1608/1608 [==============================] - 51s 31ms/step - loss: 0.4177 - accuracy: 0.8582 - val_loss: 1.0572 - val_accuracy: 0.7007\n",
            "Epoch 14/30\n",
            "1608/1608 [==============================] - 51s 31ms/step - loss: 0.3922 - accuracy: 0.8663 - val_loss: 1.0662 - val_accuracy: 0.7007\n",
            "Epoch 15/30\n",
            "1608/1608 [==============================] - 51s 31ms/step - loss: 0.3659 - accuracy: 0.8831 - val_loss: 1.0821 - val_accuracy: 0.6972\n",
            "Epoch 16/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.3405 - accuracy: 0.8874 - val_loss: 1.1462 - val_accuracy: 0.6866\n",
            "Epoch 17/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.3106 - accuracy: 0.9049 - val_loss: 1.1363 - val_accuracy: 0.6725\n",
            "Epoch 18/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.2653 - accuracy: 0.9192 - val_loss: 1.0837 - val_accuracy: 0.6901\n",
            "Epoch 19/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.2688 - accuracy: 0.9192 - val_loss: 1.2125 - val_accuracy: 0.6549\n",
            "Epoch 20/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.2601 - accuracy: 0.9142 - val_loss: 1.2019 - val_accuracy: 0.6866\n",
            "Epoch 21/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.2260 - accuracy: 0.9310 - val_loss: 1.2253 - val_accuracy: 0.6866\n",
            "Epoch 22/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.2025 - accuracy: 0.9490 - val_loss: 1.2971 - val_accuracy: 0.6761\n",
            "Epoch 23/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.2060 - accuracy: 0.9347 - val_loss: 1.1921 - val_accuracy: 0.6901\n",
            "Epoch 24/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.2147 - accuracy: 0.9366 - val_loss: 1.1764 - val_accuracy: 0.6972\n",
            "Epoch 25/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.1719 - accuracy: 0.9521 - val_loss: 1.2514 - val_accuracy: 0.7077\n",
            "Epoch 26/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.1834 - accuracy: 0.9415 - val_loss: 1.2547 - val_accuracy: 0.6937\n",
            "Epoch 27/30\n",
            "1608/1608 [==============================] - 49s 31ms/step - loss: 0.1806 - accuracy: 0.9366 - val_loss: 1.2827 - val_accuracy: 0.6761\n",
            "Epoch 28/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.1569 - accuracy: 0.9502 - val_loss: 1.2936 - val_accuracy: 0.7042\n",
            "Epoch 29/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.1662 - accuracy: 0.9465 - val_loss: 1.2915 - val_accuracy: 0.6972\n",
            "Epoch 30/30\n",
            "1608/1608 [==============================] - 50s 31ms/step - loss: 0.1498 - accuracy: 0.9571 - val_loss: 1.3270 - val_accuracy: 0.6831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FVC83cg9VS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1877ecb-4b78-442f-8d00-b74ecc8e21d8"
      },
      "source": [
        "predicted = model_bi_lstm.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7044776119402985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QRNl0wXDnNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "20e3ec2e-69af-4f2a-9cad-b10694391e71"
      },
      "source": [
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2514ffcc88>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmM9BRmpEC8G",
        "colab_type": "text"
      },
      "source": [
        "# **5. Hybrid  - CNN + LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26tWGg6Dx1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix) = create_data_for_model(vocab_size = 12000, max_len = 35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0oyH3YETkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_hybrid_model(max_len,vocab_size):\n",
        "\n",
        "  pooling_layers = []\n",
        "\n",
        "\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=False,weights = [embeddings_matrix],trainable = False)(inp1)\n",
        "  dp = Dropout(0.3)(embedding_layer)\n",
        "\n",
        "  x1 = Conv1D(filters=36,kernel_size=1,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 9,stride = 1)(x1))\n",
        "\n",
        "  x2 = Conv1D(filters=36,kernel_size=2,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 10,stride = 1)(x2))\n",
        "\n",
        "  x3 = Conv1D(filters=36,kernel_size=3,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 11,stride = 1)(x3))\n",
        "\n",
        "  z = Concatenate(axis = 2)(pooling_layers)\n",
        "  z = Reshape(((108,-1)))(z)\n",
        "\n",
        "  lstm_layer1 = (LSTM(100,return_sequences = False))(z)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  hybrid_model = Model(inputs = inp1,outputs = out1)\n",
        "  hybrid_model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  hybrid_model.summary()\n",
        "  return hybrid_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WGkqPgcHS00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "932103bf-d703-4d02-ac52-c6e18658381d"
      },
      "source": [
        "hybrid_model = create_hybrid_model(35,12000)\n",
        "hist = hybrid_model.fit(padded_train,y_train,epochs = 35, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=26, strides=1)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=25, strides=1)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=24, strides=1)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 35)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 35, 50)       600050      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 35, 50)       0           embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 35, 36)       1836        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 34, 36)       3636        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 33, 36)       5436        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, 10, 36)       0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling1D) (None, 10, 36)       0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling1D) (None, 10, 36)       0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 108)      0           max_pooling1d_9[0][0]            \n",
            "                                                                 max_pooling1d_10[0][0]           \n",
            "                                                                 max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 108, 10)      0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 100)          44400       reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 11)           1111        lstm_6[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 656,469\n",
            "Trainable params: 56,419\n",
            "Non-trainable params: 600,050\n",
            "__________________________________________________________________________________________________\n",
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/35\n",
            "1608/1608 [==============================] - 59s 37ms/step - loss: 2.3279 - accuracy: 0.1362 - val_loss: 2.1403 - val_accuracy: 0.2711\n",
            "Epoch 2/35\n",
            "1608/1608 [==============================] - 57s 35ms/step - loss: 2.0394 - accuracy: 0.2935 - val_loss: 1.7589 - val_accuracy: 0.3697\n",
            "Epoch 3/35\n",
            "1608/1608 [==============================] - 57s 35ms/step - loss: 1.7939 - accuracy: 0.3818 - val_loss: 1.6135 - val_accuracy: 0.4613\n",
            "Epoch 4/35\n",
            "1608/1608 [==============================] - 57s 35ms/step - loss: 1.5913 - accuracy: 0.4782 - val_loss: 1.4447 - val_accuracy: 0.5352\n",
            "Epoch 5/35\n",
            "1608/1608 [==============================] - 57s 35ms/step - loss: 1.4711 - accuracy: 0.5093 - val_loss: 1.4824 - val_accuracy: 0.4859\n",
            "Epoch 6/35\n",
            "1608/1608 [==============================] - 58s 36ms/step - loss: 1.4250 - accuracy: 0.5274 - val_loss: 1.4260 - val_accuracy: 0.5176\n",
            "Epoch 7/35\n",
            "1608/1608 [==============================] - 57s 35ms/step - loss: 1.3678 - accuracy: 0.5404 - val_loss: 1.3959 - val_accuracy: 0.5317\n",
            "Epoch 8/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.3086 - accuracy: 0.5553 - val_loss: 1.3748 - val_accuracy: 0.5352\n",
            "Epoch 9/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.2541 - accuracy: 0.5765 - val_loss: 1.3610 - val_accuracy: 0.5387\n",
            "Epoch 10/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.2296 - accuracy: 0.5883 - val_loss: 1.3540 - val_accuracy: 0.5282\n",
            "Epoch 11/35\n",
            "1608/1608 [==============================] - 55s 34ms/step - loss: 1.1948 - accuracy: 0.6039 - val_loss: 1.3828 - val_accuracy: 0.5387\n",
            "Epoch 12/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.1563 - accuracy: 0.6188 - val_loss: 1.3915 - val_accuracy: 0.5246\n",
            "Epoch 13/35\n",
            "1608/1608 [==============================] - 55s 34ms/step - loss: 1.1399 - accuracy: 0.6225 - val_loss: 1.4050 - val_accuracy: 0.5634\n",
            "Epoch 14/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.0882 - accuracy: 0.6238 - val_loss: 1.3504 - val_accuracy: 0.5634\n",
            "Epoch 15/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.0898 - accuracy: 0.6524 - val_loss: 1.3465 - val_accuracy: 0.5528\n",
            "Epoch 16/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.0131 - accuracy: 0.6704 - val_loss: 1.6960 - val_accuracy: 0.5246\n",
            "Epoch 17/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.1128 - accuracy: 0.6331 - val_loss: 1.3875 - val_accuracy: 0.5493\n",
            "Epoch 18/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.9769 - accuracy: 0.6779 - val_loss: 1.3458 - val_accuracy: 0.5704\n",
            "Epoch 19/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.9737 - accuracy: 0.6710 - val_loss: 1.3388 - val_accuracy: 0.5810\n",
            "Epoch 20/35\n",
            "1608/1608 [==============================] - 55s 34ms/step - loss: 1.0311 - accuracy: 0.6604 - val_loss: 1.4055 - val_accuracy: 0.5317\n",
            "Epoch 21/35\n",
            "1608/1608 [==============================] - 55s 35ms/step - loss: 0.9822 - accuracy: 0.6754 - val_loss: 1.3625 - val_accuracy: 0.5599\n",
            "Epoch 22/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.9612 - accuracy: 0.6872 - val_loss: 1.4280 - val_accuracy: 0.5704\n",
            "Epoch 23/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.9208 - accuracy: 0.7015 - val_loss: 1.3778 - val_accuracy: 0.5528\n",
            "Epoch 24/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.8677 - accuracy: 0.7077 - val_loss: 1.4781 - val_accuracy: 0.5634\n",
            "Epoch 25/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 1.0593 - accuracy: 0.6598 - val_loss: 1.4191 - val_accuracy: 0.5599\n",
            "Epoch 26/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.9007 - accuracy: 0.7046 - val_loss: 1.4965 - val_accuracy: 0.5528\n",
            "Epoch 27/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.8681 - accuracy: 0.6996 - val_loss: 1.5262 - val_accuracy: 0.5634\n",
            "Epoch 28/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.8569 - accuracy: 0.7021 - val_loss: 1.4216 - val_accuracy: 0.5599\n",
            "Epoch 29/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.8414 - accuracy: 0.7208 - val_loss: 1.4265 - val_accuracy: 0.5493\n",
            "Epoch 30/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.8108 - accuracy: 0.7239 - val_loss: 1.4979 - val_accuracy: 0.5951\n",
            "Epoch 31/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.8817 - accuracy: 0.7021 - val_loss: 1.5178 - val_accuracy: 0.5915\n",
            "Epoch 32/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.7982 - accuracy: 0.7407 - val_loss: 1.5371 - val_accuracy: 0.5810\n",
            "Epoch 33/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.7751 - accuracy: 0.7481 - val_loss: 1.3914 - val_accuracy: 0.6021\n",
            "Epoch 34/35\n",
            "1608/1608 [==============================] - 56s 35ms/step - loss: 0.7592 - accuracy: 0.7519 - val_loss: 1.5548 - val_accuracy: 0.5141\n",
            "Epoch 35/35\n",
            "1608/1608 [==============================] - 55s 35ms/step - loss: 0.7843 - accuracy: 0.7282 - val_loss: 1.4521 - val_accuracy: 0.5845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mANRef43O-DD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34acddfc-8063-489f-8020-096d27e04d49"
      },
      "source": [
        "predicted = hybrid_model.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5671641791044776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBsedBPllfL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_hybrid_model1(max_len,vocab_size):\n",
        "\n",
        "  pooling_layers = []\n",
        "\n",
        "\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=False,weights = [embeddings_matrix],trainable = False)(inp1)\n",
        "  dp = Dropout(0.3)(embedding_layer)\n",
        "\n",
        "  x1 = Conv1D(filters=36,kernel_size=5,activation = 'relu')(dp)\n",
        "  mp = MaxPool1D(pool_size=4,stride = 1)(x1)\n",
        "\n",
        "  #x2 = Conv1D(filters=36,kernel_size=2,activation = 'relu')(dp)\n",
        "  #pooling_layers.append(MaxPool1D(pool_size=max_len - 10,stride = 1)(x2))\n",
        "\n",
        "  #x3 = Conv1D(filters=36,kernel_size=3,activation = 'relu')(dp)\n",
        "  #pooling_layers.append(MaxPool1D(pool_size=max_len - 11,stride = 1)(x3))\n",
        "\n",
        "  #z = Concatenate(axis = 2)(pooling_layers)\n",
        "  #z = Reshape(((108,-1)))(z)\n",
        "\n",
        "  lstm_layer1 = (LSTM(100,return_sequences = False))(mp)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  hybrid_model = Model(inputs = inp1,outputs = out1)\n",
        "  hybrid_model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  hybrid_model.summary()\n",
        "  return hybrid_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LlB9UgkWnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdca1f75-a0a3-4da8-b8f7-ecddf2233a04"
      },
      "source": [
        "hybrid_model = create_hybrid_model1(35,12000)\n",
        "hist = hybrid_model.fit(padded_train,y_train,epochs = 35, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4, strides=1)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 35)                0         \n",
            "_________________________________________________________________\n",
            "embedding_9 (Embedding)      (None, 35, 50)            600050    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 35, 50)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 31, 36)            9036      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 28, 36)            0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 100)               54800     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 11)                1111      \n",
            "=================================================================\n",
            "Total params: 664,997\n",
            "Trainable params: 64,947\n",
            "Non-trainable params: 600,050\n",
            "_________________________________________________________________\n",
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/35\n",
            "1608/1608 [==============================] - 17s 11ms/step - loss: 2.2472 - accuracy: 0.1959 - val_loss: 2.0529 - val_accuracy: 0.2852\n",
            "Epoch 2/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 1.9604 - accuracy: 0.3228 - val_loss: 1.6864 - val_accuracy: 0.4120\n",
            "Epoch 3/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 1.7123 - accuracy: 0.4291 - val_loss: 1.6995 - val_accuracy: 0.4261\n",
            "Epoch 4/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 1.5614 - accuracy: 0.4776 - val_loss: 1.5437 - val_accuracy: 0.4894\n",
            "Epoch 5/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 1.4375 - accuracy: 0.5087 - val_loss: 1.5021 - val_accuracy: 0.4965\n",
            "Epoch 6/35\n",
            "1608/1608 [==============================] - 17s 11ms/step - loss: 1.3333 - accuracy: 0.5547 - val_loss: 1.3180 - val_accuracy: 0.5845\n",
            "Epoch 7/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 1.2712 - accuracy: 0.5871 - val_loss: 1.2690 - val_accuracy: 0.5563\n",
            "Epoch 8/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 1.1582 - accuracy: 0.6262 - val_loss: 1.3081 - val_accuracy: 0.5775\n",
            "Epoch 9/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 1.0831 - accuracy: 0.6530 - val_loss: 1.2648 - val_accuracy: 0.5845\n",
            "Epoch 10/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 1.0032 - accuracy: 0.6660 - val_loss: 1.1815 - val_accuracy: 0.6092\n",
            "Epoch 11/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 0.9387 - accuracy: 0.6965 - val_loss: 1.1154 - val_accuracy: 0.6479\n",
            "Epoch 12/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.8509 - accuracy: 0.7146 - val_loss: 1.2777 - val_accuracy: 0.6162\n",
            "Epoch 13/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.8410 - accuracy: 0.7257 - val_loss: 1.1228 - val_accuracy: 0.6444\n",
            "Epoch 14/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.7650 - accuracy: 0.7444 - val_loss: 1.0838 - val_accuracy: 0.6620\n",
            "Epoch 15/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.7480 - accuracy: 0.7500 - val_loss: 1.2040 - val_accuracy: 0.6162\n",
            "Epoch 16/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.6997 - accuracy: 0.7668 - val_loss: 1.1898 - val_accuracy: 0.6408\n",
            "Epoch 17/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.6428 - accuracy: 0.7886 - val_loss: 1.1880 - val_accuracy: 0.6338\n",
            "Epoch 18/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.6215 - accuracy: 0.7836 - val_loss: 1.1777 - val_accuracy: 0.6303\n",
            "Epoch 19/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.5896 - accuracy: 0.8035 - val_loss: 1.2112 - val_accuracy: 0.6373\n",
            "Epoch 20/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.5674 - accuracy: 0.8122 - val_loss: 1.1810 - val_accuracy: 0.6232\n",
            "Epoch 21/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.5389 - accuracy: 0.8252 - val_loss: 1.1382 - val_accuracy: 0.6690\n",
            "Epoch 22/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.5067 - accuracy: 0.8340 - val_loss: 1.2111 - val_accuracy: 0.6549\n",
            "Epoch 23/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.4628 - accuracy: 0.8464 - val_loss: 1.1630 - val_accuracy: 0.6549\n",
            "Epoch 24/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.4346 - accuracy: 0.8551 - val_loss: 1.2721 - val_accuracy: 0.6549\n",
            "Epoch 25/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.4293 - accuracy: 0.8563 - val_loss: 1.3361 - val_accuracy: 0.6479\n",
            "Epoch 26/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.4408 - accuracy: 0.8520 - val_loss: 1.2595 - val_accuracy: 0.6479\n",
            "Epoch 27/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.3946 - accuracy: 0.8626 - val_loss: 1.2421 - val_accuracy: 0.6479\n",
            "Epoch 28/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.4045 - accuracy: 0.8613 - val_loss: 1.2257 - val_accuracy: 0.6725\n",
            "Epoch 29/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 0.3430 - accuracy: 0.8818 - val_loss: 1.3121 - val_accuracy: 0.6620\n",
            "Epoch 30/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.3404 - accuracy: 0.8800 - val_loss: 1.3912 - val_accuracy: 0.6373\n",
            "Epoch 31/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.3731 - accuracy: 0.8694 - val_loss: 1.2960 - val_accuracy: 0.6303\n",
            "Epoch 32/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.3428 - accuracy: 0.8850 - val_loss: 1.4320 - val_accuracy: 0.6232\n",
            "Epoch 33/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.2880 - accuracy: 0.9011 - val_loss: 1.3536 - val_accuracy: 0.6585\n",
            "Epoch 34/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.3076 - accuracy: 0.8943 - val_loss: 1.4722 - val_accuracy: 0.6303\n",
            "Epoch 35/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.2814 - accuracy: 0.9049 - val_loss: 1.4327 - val_accuracy: 0.6373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYh2BWvNmojH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d45cbe7-b847-43cf-f848-af03d1de0426"
      },
      "source": [
        "predicted = hybrid_model.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6626865671641791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDrG7gVEpC8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85117c59-d90c-4ae3-c4ac-efbcb1a67632"
      },
      "source": [
        "hybrid_model.fit(padded_train,y_train,epochs = 35, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 0.1457 - accuracy: 0.9403 - val_loss: 1.7806 - val_accuracy: 0.6655\n",
            "Epoch 2/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1222 - accuracy: 0.9552 - val_loss: 1.6950 - val_accuracy: 0.6831\n",
            "Epoch 3/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 0.1643 - accuracy: 0.9391 - val_loss: 1.7950 - val_accuracy: 0.6585\n",
            "Epoch 4/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 0.1653 - accuracy: 0.9447 - val_loss: 1.6943 - val_accuracy: 0.6761\n",
            "Epoch 5/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1541 - accuracy: 0.9440 - val_loss: 1.6101 - val_accuracy: 0.6866\n",
            "Epoch 6/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1435 - accuracy: 0.9515 - val_loss: 1.6901 - val_accuracy: 0.6655\n",
            "Epoch 7/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1612 - accuracy: 0.9484 - val_loss: 1.6022 - val_accuracy: 0.6831\n",
            "Epoch 8/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1510 - accuracy: 0.9440 - val_loss: 1.7187 - val_accuracy: 0.6444\n",
            "Epoch 9/35\n",
            "1608/1608 [==============================] - 17s 10ms/step - loss: 0.1231 - accuracy: 0.9496 - val_loss: 1.7446 - val_accuracy: 0.6408\n",
            "Epoch 10/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1451 - accuracy: 0.9471 - val_loss: 1.8496 - val_accuracy: 0.6479\n",
            "Epoch 11/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1452 - accuracy: 0.9465 - val_loss: 1.7242 - val_accuracy: 0.6690\n",
            "Epoch 12/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1178 - accuracy: 0.9527 - val_loss: 1.7193 - val_accuracy: 0.6761\n",
            "Epoch 13/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1253 - accuracy: 0.9484 - val_loss: 1.8340 - val_accuracy: 0.6408\n",
            "Epoch 14/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1493 - accuracy: 0.9471 - val_loss: 1.8192 - val_accuracy: 0.6620\n",
            "Epoch 15/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1337 - accuracy: 0.9496 - val_loss: 1.8310 - val_accuracy: 0.6549\n",
            "Epoch 16/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1117 - accuracy: 0.9571 - val_loss: 1.6815 - val_accuracy: 0.6796\n",
            "Epoch 17/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1390 - accuracy: 0.9484 - val_loss: 1.7739 - val_accuracy: 0.6408\n",
            "Epoch 18/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1188 - accuracy: 0.9571 - val_loss: 1.7597 - val_accuracy: 0.6725\n",
            "Epoch 19/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1273 - accuracy: 0.9602 - val_loss: 1.7921 - val_accuracy: 0.6761\n",
            "Epoch 20/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1607 - accuracy: 0.9490 - val_loss: 1.7581 - val_accuracy: 0.6514\n",
            "Epoch 21/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1396 - accuracy: 0.9465 - val_loss: 1.7635 - val_accuracy: 0.6479\n",
            "Epoch 22/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1440 - accuracy: 0.9509 - val_loss: 1.7043 - val_accuracy: 0.6690\n",
            "Epoch 23/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1234 - accuracy: 0.9527 - val_loss: 1.7497 - val_accuracy: 0.6655\n",
            "Epoch 24/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1235 - accuracy: 0.9490 - val_loss: 1.7753 - val_accuracy: 0.6761\n",
            "Epoch 25/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1252 - accuracy: 0.9552 - val_loss: 1.8294 - val_accuracy: 0.6620\n",
            "Epoch 26/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1405 - accuracy: 0.9490 - val_loss: 1.7893 - val_accuracy: 0.6831\n",
            "Epoch 27/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1167 - accuracy: 0.9552 - val_loss: 1.9199 - val_accuracy: 0.6690\n",
            "Epoch 28/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1315 - accuracy: 0.9509 - val_loss: 1.7824 - val_accuracy: 0.6303\n",
            "Epoch 29/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1334 - accuracy: 0.9471 - val_loss: 1.8643 - val_accuracy: 0.6514\n",
            "Epoch 30/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1310 - accuracy: 0.9583 - val_loss: 1.9207 - val_accuracy: 0.6585\n",
            "Epoch 31/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1329 - accuracy: 0.9502 - val_loss: 1.8004 - val_accuracy: 0.6585\n",
            "Epoch 32/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1173 - accuracy: 0.9571 - val_loss: 1.7415 - val_accuracy: 0.6866\n",
            "Epoch 33/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1243 - accuracy: 0.9521 - val_loss: 1.7574 - val_accuracy: 0.6761\n",
            "Epoch 34/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.0996 - accuracy: 0.9639 - val_loss: 1.7160 - val_accuracy: 0.6761\n",
            "Epoch 35/35\n",
            "1608/1608 [==============================] - 16s 10ms/step - loss: 0.1188 - accuracy: 0.9558 - val_loss: 1.8353 - val_accuracy: 0.6901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f25130ab358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8UWdBTpMEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
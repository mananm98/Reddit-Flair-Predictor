{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part - 3 Building Flair Detector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gosbLEJ-i03f",
        "QFdadyGWkMIV",
        "BVP8GKXskitI",
        "GDhrTvZjk5LJ",
        "ud9Dy7TcleC9",
        "IMnHcBs5lhFv",
        "AKuReBoHmUjc",
        "PnuAkOLinU3I",
        "koXHKZkEmWXV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7HrzWHv0Xns",
        "colab_type": "code",
        "outputId": "a2d3c65d-6308-4881-a653-b11ed94e9ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DSVINrXsKf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd2ad406-e0d9-466f-9e78-94824708e374"
      },
      "source": [
        "\n",
        "from IPython.display import HTML\n",
        "HTML('''<script>\n",
        "code_show_err=false; \n",
        "function code_toggle_err() {\n",
        " if (code_show_err){\n",
        " $('div.output_stderr').hide();\n",
        " } else {\n",
        " $('div.output_stderr').show();\n",
        " }\n",
        " code_show_err = !code_show_err\n",
        "} \n",
        "$( document ).ready(code_toggle_err);\n",
        "</script>\n",
        "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<script>\n",
              "code_show_err=false; \n",
              "function code_toggle_err() {\n",
              " if (code_show_err){\n",
              " $('div.output_stderr').hide();\n",
              " } else {\n",
              " $('div.output_stderr').show();\n",
              " }\n",
              " code_show_err = !code_show_err\n",
              "} \n",
              "$( document ).ready(code_toggle_err);\n",
              "</script>\n",
              "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLoiaK4U0ssB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Data\n",
        "data = pd.read_csv('/content/drive/My Drive/reddit-data-cleaned.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEylSp_l2k6-",
        "colab_type": "code",
        "outputId": "e7c8baef-0779-4ffc-9c2a-ffa181f1e27a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "      <th>body</th>\n",
              "      <th>url</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>comments</th>\n",
              "      <th>flair</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>delhi govt source names cm arvind kejriwal dep...</td>\n",
              "      <td>302</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ani status</td>\n",
              "      <td>30</td>\n",
              "      <td>beyond petty inclusion delhi government school...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>f7ogd8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delhi ap singh advocate delhi gang rape convic...</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ani status</td>\n",
              "      <td>22</td>\n",
              "      <td>hunch guy try expose loophole legal system nev...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>flgvah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>supreme court verdict sc st quota create polit...</td>\n",
              "      <td>106</td>\n",
              "      <td>NaN</td>\n",
              "      <td>scroll article supreme courts verdict sc st qu...</td>\n",
              "      <td>47</td>\n",
              "      <td>muslim reservation two distraction use indian ...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>f1o839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>entrance exam schedule may</td>\n",
              "      <td>9</td>\n",
              "      <td>clat ailet neet jee postpone two week would ab...</td>\n",
              "      <td>india comments fvcvo entrance exams scheduled may</td>\n",
              "      <td>3</td>\n",
              "      <td>bachega india tabhi toh padhega india gand mar...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>fvcvo1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>advisory schedule international mercial passen...</td>\n",
              "      <td>36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>pib india status</td>\n",
              "      <td>4</td>\n",
              "      <td>oh boy chalo bhaisahab sabji ka dukaan main da...</td>\n",
              "      <td>Scheduled</td>\n",
              "      <td>fl8zf5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  score  ...      flair      id\n",
              "0  delhi govt source names cm arvind kejriwal dep...    302  ...  Scheduled  f7ogd8\n",
              "1  delhi ap singh advocate delhi gang rape convic...     17  ...  Scheduled  flgvah\n",
              "2  supreme court verdict sc st quota create polit...    106  ...  Scheduled  f1o839\n",
              "3                         entrance exam schedule may      9  ...  Scheduled  fvcvo1\n",
              "4  advisory schedule international mercial passen...     36  ...  Scheduled  fl8zf5\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsVoYfNSijYu",
        "colab_type": "text"
      },
      "source": [
        "## **1. Bag of words Model on conventional ML algorithms**\n",
        "---\n",
        "- We cannot input text directly to machine learning models. We need to convert the text to a vector of numbers, this step is called **Feature extraction**\n",
        "  \n",
        "    \n",
        "- For this we are going to use B.O.W (Bag of words) model, It focuses only on the occurence of words. The sentence structure, context, order of words is lost in B.O.W model.\n",
        "\n",
        "\n",
        "- First we will convert each document in corpus to TF-IDF vector\n",
        "\n",
        "  \n",
        "- We will input these vectors to Machine Learning models like Naive-Bayes, Support-Vector-Machine, Logistic-Regression, Random-Forest \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gosbLEJ-i03f",
        "colab_type": "text"
      },
      "source": [
        "### **Preparing data for model**\n",
        "---\n",
        "\n",
        "- It is possible that some features from the dataset may perform better than others. For instance, Only using **Title** for our model may give better accuracy than using only **url**, or maybe a combination of such features might result in a better accuracy. \n",
        "\n",
        "  \n",
        "- this is hard to guess at the moment, so I plan to try out different combinations of inputs from the dataset to get the best accuracy:- Title, url, comments, (Title + url + comments), (Title + url) , etc.\n",
        "\n",
        "  \n",
        "- lets see which performs the best, we will use those features in our final model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_RFjrcViiwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(columns):\n",
        "    # Prepares Train and test sets for models\n",
        "        \n",
        "    if len(columns) > 2:\n",
        "        df = data[columns].fillna(\"\")\n",
        "        columns.remove('flair')\n",
        "        X = df[columns].apply(lambda x : ' '.join(x),axis = 1)                     \n",
        "       \n",
        "    else :\n",
        "        df = data[columns].dropna()\n",
        "        X = df[columns[0]]\n",
        "        \n",
        "    X = X.values          # X - input\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    Y = le.fit_transform(df['flair'])    # Y - target_labels\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)   # ( 85 : 15 )\n",
        "    return (X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFdadyGWkMIV",
        "colab_type": "text"
      },
      "source": [
        "### **1.1 Title**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-_gyjAAiito",
        "colab_type": "code",
        "outputId": "d8b1193b-1277-4a2c-aaf2-29dfbbb0ee4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Extracting 'Title' from dataset\n",
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVP8GKXskitI",
        "colab_type": "text"
      },
      "source": [
        "### Data is ready now we'll apply it to different classifiers\n",
        "---\n",
        "1. Linear SVC\n",
        "2. Naive - Bayes\n",
        "3. Logistic Regression\n",
        "4. Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fenFYNOiiiw",
        "colab_type": "code",
        "outputId": "5c387b22-fc2d-4b65-8ae2-a1017cf8d59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.2)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 0.9)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100,max_depth=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        # tf-idf vectorisation\n",
        "                    (clf[0],clf[1])                                            # estimator\n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7194029850746269\n",
            "\n",
            "Naive - Bayes   ----->   0.7074626865671642\n",
            "\n",
            "LogisticRegression   ----->   0.746268656716418\n",
            "\n",
            "Random Forest Classifier   ----->   0.7134328358208956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDhrTvZjk5LJ",
        "colab_type": "text"
      },
      "source": [
        "### **1.2 URL** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqjUYJd3iiZb",
        "colab_type": "code",
        "outputId": "02951045-0347-43e0-bc41-7886a06de2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Extracting url column from dataset\n",
        "(X_train, X_test, y_train, y_test) = prepare_data(['url','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1890,)\n",
            "y_train shape  =  (1890,)\n",
            "X_test shape  =  (334,)\n",
            "y_test shape  =  (334,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daogzFoPiiOA",
        "colab_type": "code",
        "outputId": "71317f51-8b4e-485c-f193-4a07225e75f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 25,solver='saga',penalty='l1',multi_class='multinomial')) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100,max_depth=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.5269461077844312\n",
            "\n",
            "Naive - Bayes   ----->   0.4431137724550898\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression   ----->   0.5239520958083832\n",
            "\n",
            "Random Forest Classifier   ----->   0.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud9Dy7TcleC9",
        "colab_type": "text"
      },
      "source": [
        "### **1.3 Comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGUvrfmflgaV",
        "colab_type": "code",
        "outputId": "dac06099-7b23-4a4f-8546-ec0fbbf88513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['comments','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1706,)\n",
            "y_train shape  =  (1706,)\n",
            "X_test shape  =  (302,)\n",
            "y_test shape  =  (302,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwCy8srNlgHS",
        "colab_type": "code",
        "outputId": "efe8703d-b2b4-4755-9e4d-56b5969672dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=1)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 10)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.5132450331125827\n",
            "\n",
            "Naive - Bayes   ----->   0.4105960264900662\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression   ----->   0.49337748344370863\n",
            "\n",
            "Random Forest Classifier   ----->   0.423841059602649\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMnHcBs5lhFv",
        "colab_type": "text"
      },
      "source": [
        "### **1.4 Title + url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjlMDQXBmTy7",
        "colab_type": "code",
        "outputId": "febffe9e-5518-4c27-b366-dee5e9b8c20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','url','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFJ3In98mThE",
        "colab_type": "code",
        "outputId": "5e23c5aa-1fc9-4996-bcb2-4f0d74fc3fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 0.9)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7283582089552239\n",
            "\n",
            "Naive - Bayes   ----->   0.6925373134328359\n",
            "\n",
            "LogisticRegression   ----->   0.7283582089552239\n",
            "\n",
            "Random Forest Classifier   ----->   0.7074626865671642\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKuReBoHmUjc",
        "colab_type": "text"
      },
      "source": [
        "### **1.5 Title + comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5hT1sUmVe8",
        "colab_type": "code",
        "outputId": "2649ae2a-ad1f-4e91-841d-fbf6ceec0157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','comments','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMRlPaffmV15",
        "colab_type": "code",
        "outputId": "517e4d75-8588-40a5-daca-23ccab31d93f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.6)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 150)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7432835820895523\n",
            "\n",
            "Naive - Bayes   ----->   0.5671641791044776\n",
            "\n",
            "LogisticRegression   ----->   0.7194029850746269\n",
            "\n",
            "Random Forest Classifier   ----->   0.7134328358208956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnuAkOLinU3I",
        "colab_type": "text"
      },
      "source": [
        "### **1.6 Title + body**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxvKsET6nVp-",
        "colab_type": "code",
        "outputId": "26182970-6726-4208-f92f-6c36107ce639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','body','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxUjKI29nWGU",
        "colab_type": "code",
        "outputId": "b25f2630-9c4a-4edc-88e8-2fad3ea6bb57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 200)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.8238805970149253\n",
            "\n",
            "Naive - Bayes   ----->   0.6716417910447762\n",
            "\n",
            "LogisticRegression   ----->   0.7970149253731343\n",
            "\n",
            "Random Forest Classifier   ----->   0.7611940298507462\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koXHKZkEmWXV",
        "colab_type": "text"
      },
      "source": [
        "### **1.7 Title + comments + url**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKrWvk4WnUWu",
        "colab_type": "code",
        "outputId": "d0decdbe-eadb-4bcf-f65c-af21bcc1c4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','url','comments','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNUcG1lVnUAC",
        "colab_type": "code",
        "outputId": "b056ef93-02a1-4896-8370-3a6272138f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=3)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 150)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test)\n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test)\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.7761194029850746\n",
            "\n",
            "Naive - Bayes   ----->   0.5940298507462687\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression   ----->   0.7343283582089553\n",
            "\n",
            "Random Forest Classifier   ----->   0.7134328358208956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ3bcnlxnWnj",
        "colab_type": "text"
      },
      "source": [
        "###  **1.8 Title + body + url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5FeEtE7oIhd",
        "colab_type": "code",
        "outputId": "5a9172c2-5813-4c2a-e9b1-3406493f011d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = prepare_data(['title','body','url','flair'])\n",
        "print(\"X_train shape  = \",X_train.shape)\n",
        "print(\"y_train shape  = \",y_train.shape)\n",
        "print(\"X_test shape  = \",X_test.shape)\n",
        "print(\"y_test shape  = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  =  (1892,)\n",
            "y_train shape  =  (1892,)\n",
            "X_test shape  =  (335,)\n",
            "y_test shape  =  (335,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WahGxyEKoIMI",
        "colab_type": "code",
        "outputId": "683b3743-a321-4db7-a134-44be95413395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "classifiers = [ ('LinearSVC',LinearSVC(loss='hinge',C=0.7)) , ('Naive - Bayes',MultinomialNB() ),('LogisticRegression' ,LogisticRegression(C = 9,max_iter = 200)) ,('Random Forest Classifier' ,RandomForestClassifier(n_estimators=100)) ]\n",
        "for clf in classifiers:\n",
        "    \n",
        "    text_clf = Pipeline([('tfidf',TfidfVectorizer(ngram_range=(1,2))),        \n",
        "                    (clf[0],clf[1])                                            \n",
        "                   ])\n",
        "    \n",
        "    text_clf.fit(X_train,y_train)\n",
        "    predicted =  text_clf.predict(X_test) \n",
        "    accuracy = np.sum(predicted == y_test)/len(y_test) # t = b = u\n",
        "    print(clf[0],'  ----->  ',accuracy,end = '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC   ----->   0.8238805970149253\n",
            "\n",
            "Naive - Bayes   ----->   0.6716417910447762\n",
            "\n",
            "LogisticRegression   ----->   0.7970149253731343\n",
            "\n",
            "Random Forest Classifier   ----->   0.7880597014925373\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzcoabkTphWX",
        "colab_type": "text"
      },
      "source": [
        "#### ***Models perform the best when we use features ( Title + Body + url )***\n",
        "- Now we will explore deep-learning approaches for text classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M6GFFMCtRTW",
        "colab_type": "text"
      },
      "source": [
        "# ***2. Deep Learning Models***\n",
        "- We will be considering the following deep learning methods for text classification\n",
        " 1. CNN + Word Embeddings\n",
        " 2. LSTMs\n",
        " 3. Stacked LSTMS\n",
        " 4. Bidirectional LSTMs\n",
        " 5. Hybrid Model --> (CNN + LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_CAC5jEWO04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating embeddings dictionary\n",
        "embeddings_dict = {}\n",
        "with open('/content/drive/My Drive/glove.6B.50d.txt') as f: \n",
        "    for line in f:\n",
        "        word = line.split()[0]\n",
        "        embeddings_dict[word] = np.array(line.split()[1:],dtype = 'float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65fmbv8B0sEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_for_model(max_len , vocab_size = 'None'):\n",
        "      (X_train, X_test, y_train, y_test) = prepare_data(['title','body','url','flair'])\n",
        "      tk = Tokenizer()\n",
        "      tk.fit_on_texts(X_train)\n",
        "      if vocab_size != 'None':\n",
        "        tk.word_index = {w:i for w,i in tk.word_index.items() if i <= vocab_size}\n",
        "      else:\n",
        "        vocab_size = len(tk.word_index)\n",
        "\n",
        "      # Train Data - X\n",
        "      encoded_train = tk.texts_to_sequences(X_train)\n",
        "      padded_train = pad_sequences(encoded_train,maxlen = max_len, padding = 'post' , truncating = 'post')\n",
        "\n",
        "      # Test Data  - X\n",
        "      encoded_test = tk.texts_to_sequences(X_test)\n",
        "      padded_test = pad_sequences(encoded_test,maxlen = max_len,padding = 'post', truncating = 'post')\n",
        "\n",
        "      # Train Data - Y\n",
        "      one_hot = OneHotEncoder()\n",
        "      y_train = y_train.reshape((-1,1))\n",
        "      y_train = one_hot.fit_transform(y_train).toarray()\n",
        "\n",
        "      # Create embeddings matrix\n",
        "      embeddings_matrix = np.zeros((vocab_size+1,50)) \n",
        "      for word,index in tk.word_index.items():\n",
        "        if embeddings_dict.get(word) is not None:\n",
        "          embeddings_matrix[index] = embeddings_dict[word]\n",
        "     \n",
        "      return (padded_train, y_train, padded_test, y_test, embeddings_matrix, vocab_size)\n",
        "\n",
        "def give_accuracy(model,x_test,y_test,name):\n",
        "    predicted = model.predict(padded_test)\n",
        "    predicted_labels = np.argmax(predicted,axis = 1)\n",
        "    accuracy = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "    print(\"Accuracy of \" + str(name) + \" model ---> \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPaLzvdJmSe0",
        "colab_type": "text"
      },
      "source": [
        "## ***2.1 CNN + Word Embeddings***\n",
        "- (**Yoav Goldberg**, in his primer on deep learning for nlp, 2015) , comments that CNNs are effective at Text Classification because of their capability to extract important features  (like tokens or sequence of tokens) regardless of their position in text.\n",
        "\n",
        "- He also comments, in text classification, the main idea is to learn words or a group of words that are good indicators of a topic, we do not necessarily care where they might appear in a document. Convolutions and pooling layers allow model to learn such local indicators invariant to their position.\n",
        "---\n",
        "1. For each training example, we will extract word embedding vectors (Glove vectors).\n",
        "2. we will feed this as input to CNN model, Our hope is that CNN model will learn some useful filters. A useful filter might be one , that has a similar embedding vector to the vector of a distinguishing word.\n",
        "    \n",
        "      \n",
        "![alt text](https://github.com/mananm98/Reddit-Flair-Predictor/blob/master/images/cnn.JPG?raw=true)\n",
        "\n",
        "  \n",
        "    \n",
        "      \n",
        "- In the image above, the filter has very similar weights to the bigram --> cat sitting. Therefore it gives a high activation.\n",
        "\n",
        "- In the next sentence it also give a high activation because the cosine distance between the vectors cat-dog and sitting-resting is very small. This is the reason to use word embeddings, they provide us with similar representations for similar words.\n",
        "\n",
        "- So, the result is that we have learnt a filter that will give high activation whenever it comes across a bigram that means \"animal resting\". Such filters help in extracting general meanings from text \n",
        "\n",
        "- They can be used by the model to understand what our text conveys,  and consequently give correct output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wdm5y11JTw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix, vocab_len) = create_data_for_model(max_len = 300)\n",
        "def create_cnn_model(max_len, vocab_size):  \n",
        "  pooling_layers = []\n",
        "  inp = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim= vocab_size + 1,output_dim=50,weights = [embeddings_matrix],trainable = True)(inp)\n",
        "  dp = Dropout(0.3)(embedding_layer)\n",
        "\n",
        "  x1 = Conv1D(filters=36,kernel_size=1,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len ,stride = 1)(x1))\n",
        "\n",
        "  x2 = Conv1D(filters=36,kernel_size=2,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 1,stride = 1)(x2))\n",
        "\n",
        "  x3 = Conv1D(filters=36,kernel_size=3,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 2,stride = 1)(x3))\n",
        "\n",
        "  x4 = Conv1D(filters=36,kernel_size=4,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 3,stride = 1)(x4))\n",
        "\n",
        "  x5 = Conv1D(filters=36,kernel_size=5,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 4,stride = 1)(x5))\n",
        "\n",
        "  z = Concatenate(axis = 1)(pooling_layers)\n",
        "  z = Flatten()(z)\n",
        "  z = Dropout(0.2)(z)\n",
        "\n",
        "  \n",
        "  #y = Dense(16,activation = 'relu')(z)\n",
        "\n",
        "  out = Dense(11,activation = 'softmax')(z)\n",
        "\n",
        "\n",
        "  model = Model(inputs = inp,outputs = out)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjKJ_YArwqHV",
        "colab_type": "code",
        "outputId": "d4a43733-13de-4ae9-e542-34c88aad8e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "cnn_model = create_cnn_model(max_len = 300,vocab_size = vocab_len)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 300, 50)      1018350     input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 300, 50)      0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 300, 36)      1836        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 299, 36)      3636        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, 298, 36)      5436        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_42 (Conv1D)              (None, 297, 36)      7236        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_43 (Conv1D)              (None, 296, 36)      9036        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_39 (MaxPooling1D) (None, 1, 36)        0           conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_40 (MaxPooling1D) (None, 1, 36)        0           conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_41 (MaxPooling1D) (None, 1, 36)        0           conv1d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_42 (MaxPooling1D) (None, 1, 36)        0           conv1d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_43 (MaxPooling1D) (None, 1, 36)        0           conv1d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 5, 36)        0           max_pooling1d_39[0][0]           \n",
            "                                                                 max_pooling1d_40[0][0]           \n",
            "                                                                 max_pooling1d_41[0][0]           \n",
            "                                                                 max_pooling1d_42[0][0]           \n",
            "                                                                 max_pooling1d_43[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 180)          0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 180)          0           flatten_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 11)           1991        dropout_17[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,047,521\n",
            "Trainable params: 1,047,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=300, strides=1)`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=299, strides=1)`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=298, strides=1)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=297, strides=1)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=296, strides=1)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAHUKETLg-S4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7dd3f11-dd4d-4074-8984-7c65b2a3c587"
      },
      "source": [
        "history = cnn_model.fit(padded_train,y_train, epochs = 65, batch_size = 32, validation_split = 0.15)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/65\n",
            "1608/1608 [==============================] - 1s 568us/step - loss: 2.9903 - accuracy: 0.1573 - val_loss: 1.9837 - val_accuracy: 0.3521\n",
            "Epoch 2/65\n",
            "1608/1608 [==============================] - 1s 320us/step - loss: 2.3075 - accuracy: 0.2730 - val_loss: 1.6130 - val_accuracy: 0.4859\n",
            "Epoch 3/65\n",
            "1608/1608 [==============================] - 1s 331us/step - loss: 1.9138 - accuracy: 0.3750 - val_loss: 1.3694 - val_accuracy: 0.6021\n",
            "Epoch 4/65\n",
            "1608/1608 [==============================] - 1s 330us/step - loss: 1.6280 - accuracy: 0.4925 - val_loss: 1.0910 - val_accuracy: 0.6972\n",
            "Epoch 5/65\n",
            "1608/1608 [==============================] - 1s 325us/step - loss: 1.3683 - accuracy: 0.5647 - val_loss: 0.9278 - val_accuracy: 0.7359\n",
            "Epoch 6/65\n",
            "1608/1608 [==============================] - 1s 324us/step - loss: 1.1449 - accuracy: 0.6418 - val_loss: 0.8603 - val_accuracy: 0.7394\n",
            "Epoch 7/65\n",
            "1608/1608 [==============================] - 0s 308us/step - loss: 0.9857 - accuracy: 0.6891 - val_loss: 0.7696 - val_accuracy: 0.7606\n",
            "Epoch 8/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.9443 - accuracy: 0.7040 - val_loss: 0.7231 - val_accuracy: 0.7746\n",
            "Epoch 9/65\n",
            "1608/1608 [==============================] - 0s 310us/step - loss: 0.8165 - accuracy: 0.7488 - val_loss: 0.6899 - val_accuracy: 0.7852\n",
            "Epoch 10/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.7678 - accuracy: 0.7624 - val_loss: 0.6780 - val_accuracy: 0.7887\n",
            "Epoch 11/65\n",
            "1608/1608 [==============================] - 1s 314us/step - loss: 0.6971 - accuracy: 0.7792 - val_loss: 0.6506 - val_accuracy: 0.8028\n",
            "Epoch 12/65\n",
            "1608/1608 [==============================] - 1s 318us/step - loss: 0.6516 - accuracy: 0.8047 - val_loss: 0.6553 - val_accuracy: 0.8099\n",
            "Epoch 13/65\n",
            "1608/1608 [==============================] - 0s 310us/step - loss: 0.5868 - accuracy: 0.8197 - val_loss: 0.6467 - val_accuracy: 0.8099\n",
            "Epoch 14/65\n",
            "1608/1608 [==============================] - 1s 314us/step - loss: 0.5469 - accuracy: 0.8228 - val_loss: 0.6378 - val_accuracy: 0.8099\n",
            "Epoch 15/65\n",
            "1608/1608 [==============================] - 0s 310us/step - loss: 0.5293 - accuracy: 0.8364 - val_loss: 0.6332 - val_accuracy: 0.8239\n",
            "Epoch 16/65\n",
            "1608/1608 [==============================] - 1s 315us/step - loss: 0.5293 - accuracy: 0.8364 - val_loss: 0.6358 - val_accuracy: 0.8204\n",
            "Epoch 17/65\n",
            "1608/1608 [==============================] - 1s 311us/step - loss: 0.4805 - accuracy: 0.8588 - val_loss: 0.6543 - val_accuracy: 0.8169\n",
            "Epoch 18/65\n",
            "1608/1608 [==============================] - 1s 318us/step - loss: 0.4451 - accuracy: 0.8613 - val_loss: 0.6568 - val_accuracy: 0.8134\n",
            "Epoch 19/65\n",
            "1608/1608 [==============================] - 1s 324us/step - loss: 0.4450 - accuracy: 0.8657 - val_loss: 0.6432 - val_accuracy: 0.8239\n",
            "Epoch 20/65\n",
            "1608/1608 [==============================] - 1s 319us/step - loss: 0.4094 - accuracy: 0.8756 - val_loss: 0.6458 - val_accuracy: 0.8134\n",
            "Epoch 21/65\n",
            "1608/1608 [==============================] - 1s 312us/step - loss: 0.3853 - accuracy: 0.8856 - val_loss: 0.6487 - val_accuracy: 0.8063\n",
            "Epoch 22/65\n",
            "1608/1608 [==============================] - 1s 313us/step - loss: 0.3574 - accuracy: 0.8918 - val_loss: 0.6424 - val_accuracy: 0.8099\n",
            "Epoch 23/65\n",
            "1608/1608 [==============================] - 1s 316us/step - loss: 0.3451 - accuracy: 0.8961 - val_loss: 0.6431 - val_accuracy: 0.8275\n",
            "Epoch 24/65\n",
            "1608/1608 [==============================] - 1s 315us/step - loss: 0.3238 - accuracy: 0.9005 - val_loss: 0.6553 - val_accuracy: 0.8275\n",
            "Epoch 25/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.3263 - accuracy: 0.9036 - val_loss: 0.6656 - val_accuracy: 0.8169\n",
            "Epoch 26/65\n",
            "1608/1608 [==============================] - 1s 324us/step - loss: 0.2816 - accuracy: 0.9179 - val_loss: 0.6517 - val_accuracy: 0.8239\n",
            "Epoch 27/65\n",
            "1608/1608 [==============================] - 0s 308us/step - loss: 0.2853 - accuracy: 0.9179 - val_loss: 0.6576 - val_accuracy: 0.8169\n",
            "Epoch 28/65\n",
            "1608/1608 [==============================] - 1s 320us/step - loss: 0.2663 - accuracy: 0.9179 - val_loss: 0.6537 - val_accuracy: 0.8204\n",
            "Epoch 29/65\n",
            "1608/1608 [==============================] - 1s 311us/step - loss: 0.2558 - accuracy: 0.9335 - val_loss: 0.6507 - val_accuracy: 0.8275\n",
            "Epoch 30/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.2446 - accuracy: 0.9266 - val_loss: 0.6703 - val_accuracy: 0.8134\n",
            "Epoch 31/65\n",
            "1608/1608 [==============================] - 0s 311us/step - loss: 0.2262 - accuracy: 0.9297 - val_loss: 0.6603 - val_accuracy: 0.8169\n",
            "Epoch 32/65\n",
            "1608/1608 [==============================] - 1s 314us/step - loss: 0.2218 - accuracy: 0.9378 - val_loss: 0.6638 - val_accuracy: 0.8204\n",
            "Epoch 33/65\n",
            "1608/1608 [==============================] - 1s 314us/step - loss: 0.2148 - accuracy: 0.9372 - val_loss: 0.6466 - val_accuracy: 0.8204\n",
            "Epoch 34/65\n",
            "1608/1608 [==============================] - 1s 311us/step - loss: 0.2023 - accuracy: 0.9403 - val_loss: 0.6415 - val_accuracy: 0.8275\n",
            "Epoch 35/65\n",
            "1608/1608 [==============================] - 0s 306us/step - loss: 0.1885 - accuracy: 0.9403 - val_loss: 0.6569 - val_accuracy: 0.8275\n",
            "Epoch 36/65\n",
            "1608/1608 [==============================] - 1s 312us/step - loss: 0.1748 - accuracy: 0.9496 - val_loss: 0.6641 - val_accuracy: 0.8415\n",
            "Epoch 37/65\n",
            "1608/1608 [==============================] - 1s 316us/step - loss: 0.1716 - accuracy: 0.9515 - val_loss: 0.6737 - val_accuracy: 0.8275\n",
            "Epoch 38/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.1706 - accuracy: 0.9540 - val_loss: 0.6727 - val_accuracy: 0.8451\n",
            "Epoch 39/65\n",
            "1608/1608 [==============================] - 1s 318us/step - loss: 0.1600 - accuracy: 0.9546 - val_loss: 0.6941 - val_accuracy: 0.8310\n",
            "Epoch 40/65\n",
            "1608/1608 [==============================] - 0s 308us/step - loss: 0.1702 - accuracy: 0.9540 - val_loss: 0.7088 - val_accuracy: 0.8380\n",
            "Epoch 41/65\n",
            "1608/1608 [==============================] - 1s 311us/step - loss: 0.1629 - accuracy: 0.9540 - val_loss: 0.7021 - val_accuracy: 0.8345\n",
            "Epoch 42/65\n",
            "1608/1608 [==============================] - 1s 314us/step - loss: 0.1507 - accuracy: 0.9540 - val_loss: 0.6876 - val_accuracy: 0.8380\n",
            "Epoch 43/65\n",
            "1608/1608 [==============================] - 1s 324us/step - loss: 0.1358 - accuracy: 0.9627 - val_loss: 0.6928 - val_accuracy: 0.8521\n",
            "Epoch 44/65\n",
            "1608/1608 [==============================] - 1s 313us/step - loss: 0.1271 - accuracy: 0.9633 - val_loss: 0.7105 - val_accuracy: 0.8239\n",
            "Epoch 45/65\n",
            "1608/1608 [==============================] - 1s 315us/step - loss: 0.1540 - accuracy: 0.9546 - val_loss: 0.6974 - val_accuracy: 0.8380\n",
            "Epoch 46/65\n",
            "1608/1608 [==============================] - 0s 310us/step - loss: 0.1416 - accuracy: 0.9708 - val_loss: 0.7057 - val_accuracy: 0.8415\n",
            "Epoch 47/65\n",
            "1608/1608 [==============================] - 1s 318us/step - loss: 0.1093 - accuracy: 0.9683 - val_loss: 0.6903 - val_accuracy: 0.8521\n",
            "Epoch 48/65\n",
            "1608/1608 [==============================] - 1s 311us/step - loss: 0.1175 - accuracy: 0.9720 - val_loss: 0.6886 - val_accuracy: 0.8556\n",
            "Epoch 49/65\n",
            "1608/1608 [==============================] - 1s 312us/step - loss: 0.1175 - accuracy: 0.9652 - val_loss: 0.6879 - val_accuracy: 0.8592\n",
            "Epoch 50/65\n",
            "1608/1608 [==============================] - 1s 313us/step - loss: 0.1301 - accuracy: 0.9596 - val_loss: 0.7121 - val_accuracy: 0.8451\n",
            "Epoch 51/65\n",
            "1608/1608 [==============================] - 1s 316us/step - loss: 0.1284 - accuracy: 0.9652 - val_loss: 0.6932 - val_accuracy: 0.8521\n",
            "Epoch 52/65\n",
            "1608/1608 [==============================] - 1s 313us/step - loss: 0.1247 - accuracy: 0.9639 - val_loss: 0.6999 - val_accuracy: 0.8556\n",
            "Epoch 53/65\n",
            "1608/1608 [==============================] - 1s 313us/step - loss: 0.1128 - accuracy: 0.9670 - val_loss: 0.6996 - val_accuracy: 0.8451\n",
            "Epoch 54/65\n",
            "1608/1608 [==============================] - 1s 311us/step - loss: 0.0971 - accuracy: 0.9720 - val_loss: 0.7109 - val_accuracy: 0.8556\n",
            "Epoch 55/65\n",
            "1608/1608 [==============================] - 1s 325us/step - loss: 0.1074 - accuracy: 0.9701 - val_loss: 0.7420 - val_accuracy: 0.8415\n",
            "Epoch 56/65\n",
            "1608/1608 [==============================] - 0s 310us/step - loss: 0.1081 - accuracy: 0.9695 - val_loss: 0.7527 - val_accuracy: 0.8451\n",
            "Epoch 57/65\n",
            "1608/1608 [==============================] - 1s 315us/step - loss: 0.0900 - accuracy: 0.9751 - val_loss: 0.7473 - val_accuracy: 0.8592\n",
            "Epoch 58/65\n",
            "1608/1608 [==============================] - 0s 310us/step - loss: 0.0907 - accuracy: 0.9733 - val_loss: 0.7547 - val_accuracy: 0.8521\n",
            "Epoch 59/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.0917 - accuracy: 0.9757 - val_loss: 0.7588 - val_accuracy: 0.8521\n",
            "Epoch 60/65\n",
            "1608/1608 [==============================] - 1s 313us/step - loss: 0.1050 - accuracy: 0.9726 - val_loss: 0.7496 - val_accuracy: 0.8486\n",
            "Epoch 61/65\n",
            "1608/1608 [==============================] - 1s 316us/step - loss: 0.0943 - accuracy: 0.9764 - val_loss: 0.7847 - val_accuracy: 0.8415\n",
            "Epoch 62/65\n",
            "1608/1608 [==============================] - 1s 319us/step - loss: 0.1063 - accuracy: 0.9695 - val_loss: 0.7569 - val_accuracy: 0.8486\n",
            "Epoch 63/65\n",
            "1608/1608 [==============================] - 1s 321us/step - loss: 0.0930 - accuracy: 0.9689 - val_loss: 0.7935 - val_accuracy: 0.8380\n",
            "Epoch 64/65\n",
            "1608/1608 [==============================] - 0s 309us/step - loss: 0.0833 - accuracy: 0.9739 - val_loss: 0.7930 - val_accuracy: 0.8451\n",
            "Epoch 65/65\n",
            "1608/1608 [==============================] - 1s 317us/step - loss: 0.0982 - accuracy: 0.9751 - val_loss: 0.7763 - val_accuracy: 0.8451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdGGd_QqwYzp",
        "colab_type": "code",
        "outputId": "17891258-a8f9-4c8e-f045-3072707dac6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "give_accuracy(cnn_model,padded_test, y_test, \"CNN\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of CNN model --->  0.8477611940298507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCkQgPqYhWIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.save_weights('/content/drive/My Drive/model_weights/model_CNN.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzPeq5kyeY7E",
        "colab_type": "text"
      },
      "source": [
        "0.8477611940298507 saved\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke1v_BxZrFOL",
        "colab_type": "code",
        "outputId": "0cd155cb-84bb-4094-8c00-3faec234d43c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(history.history['accuracy'],label = 'train_accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'validation_accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U50-6zDu4w4S",
        "colab_type": "text"
      },
      "source": [
        "# **2. LSTM**\n",
        "- The idea behind using LSTMs is to capture context and the order of text.\n",
        "\n",
        "- LSTMs can remember past information using hidden states and apply it to current task\n",
        "\n",
        "- Consider a sentence --> **The movie was not good, even though everyone was claiming it to be awesome, brilliant, extraordinary!**\n",
        "\n",
        "- To detect sentiment of this text, if we apply a CNN or linear classifier, it may predict that text is positive base on the words -  good, awesome, brilliant, extraordinary\n",
        "\n",
        "- But a LSTM might be able to recognize that due to the presence of 'not good' , 'even though' our sentence is actually negative\n",
        "\n",
        "- ***LSTMs are slow to train so I will set max_len = median length of texts(i.e 35)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuEf_lzZX2wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix, vocab_len) = create_data_for_model(max_len = 35) # taking max_len = median"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0vrDEyi4qIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "def model_lstm(max_len,vocab_size):\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=True,weights = [embeddings_matrix],trainable = False)(inp1)\n",
        "  drop_out1 = Dropout(0.3)(embedding_layer)\n",
        "  lstm_layer1 = LSTM(100,return_sequences = False)(drop_out1)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  model_lstm = Model(inputs = inp1,outputs = out1)\n",
        "  model_lstm.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model_lstm.summary()\n",
        "  return model_lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boQBFLRV4p8q",
        "colab_type": "code",
        "outputId": "f096f2f1-45af-4c34-9750-c4c821dd1d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "lstm_model = model_lstm(max_len = 35,vocab_size=vocab_len)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 35)                0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, 35, 50)            1018350   \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 35, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 11)                1111      \n",
            "=================================================================\n",
            "Total params: 1,079,861\n",
            "Trainable params: 61,511\n",
            "Non-trainable params: 1,018,350\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuX-dzBUYMU2",
        "colab_type": "code",
        "outputId": "3ea83641-a939-4d29-d642-c22c78d07fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = lstm_model.fit(padded_train,y_train,epochs = 40, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/40\n",
            "1608/1608 [==============================] - 28s 17ms/step - loss: 1.9971 - accuracy: 0.3072 - val_loss: 1.6515 - val_accuracy: 0.4507\n",
            "Epoch 2/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 1.5685 - accuracy: 0.4975 - val_loss: 1.4388 - val_accuracy: 0.5458\n",
            "Epoch 3/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 1.3986 - accuracy: 0.5572 - val_loss: 1.3682 - val_accuracy: 0.5599\n",
            "Epoch 4/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 1.2494 - accuracy: 0.5914 - val_loss: 1.3153 - val_accuracy: 0.5951\n",
            "Epoch 5/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 1.1311 - accuracy: 0.6393 - val_loss: 1.2337 - val_accuracy: 0.6338\n",
            "Epoch 6/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 1.0177 - accuracy: 0.6716 - val_loss: 1.1379 - val_accuracy: 0.6549\n",
            "Epoch 7/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.9625 - accuracy: 0.6766 - val_loss: 1.1230 - val_accuracy: 0.6444\n",
            "Epoch 8/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.8632 - accuracy: 0.7164 - val_loss: 1.1283 - val_accuracy: 0.6549\n",
            "Epoch 9/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.8239 - accuracy: 0.7307 - val_loss: 1.0701 - val_accuracy: 0.6761\n",
            "Epoch 10/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.7585 - accuracy: 0.7519 - val_loss: 1.1163 - val_accuracy: 0.6479\n",
            "Epoch 11/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.7172 - accuracy: 0.7655 - val_loss: 1.1235 - val_accuracy: 0.6549\n",
            "Epoch 12/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.6503 - accuracy: 0.7830 - val_loss: 1.0828 - val_accuracy: 0.6761\n",
            "Epoch 13/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.5907 - accuracy: 0.8091 - val_loss: 1.1440 - val_accuracy: 0.6655\n",
            "Epoch 14/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.5887 - accuracy: 0.7998 - val_loss: 1.1052 - val_accuracy: 0.6725\n",
            "Epoch 15/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.5533 - accuracy: 0.8172 - val_loss: 1.1276 - val_accuracy: 0.6796\n",
            "Epoch 16/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.5157 - accuracy: 0.8259 - val_loss: 1.1051 - val_accuracy: 0.6831\n",
            "Epoch 17/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.4858 - accuracy: 0.8321 - val_loss: 1.1407 - val_accuracy: 0.6901\n",
            "Epoch 18/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.4641 - accuracy: 0.8439 - val_loss: 1.1682 - val_accuracy: 0.6866\n",
            "Epoch 19/40\n",
            "1608/1608 [==============================] - 26s 16ms/step - loss: 0.4111 - accuracy: 0.8644 - val_loss: 1.2110 - val_accuracy: 0.6585\n",
            "Epoch 20/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.3819 - accuracy: 0.8719 - val_loss: 1.1635 - val_accuracy: 0.6620\n",
            "Epoch 21/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.3755 - accuracy: 0.8837 - val_loss: 1.2338 - val_accuracy: 0.6655\n",
            "Epoch 22/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.3558 - accuracy: 0.8787 - val_loss: 1.2459 - val_accuracy: 0.6761\n",
            "Epoch 23/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.3112 - accuracy: 0.8968 - val_loss: 1.2517 - val_accuracy: 0.6901\n",
            "Epoch 24/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.3214 - accuracy: 0.8974 - val_loss: 1.2833 - val_accuracy: 0.6831\n",
            "Epoch 25/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2988 - accuracy: 0.9030 - val_loss: 1.2246 - val_accuracy: 0.6972\n",
            "Epoch 26/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2595 - accuracy: 0.9148 - val_loss: 1.2562 - val_accuracy: 0.6866\n",
            "Epoch 27/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2417 - accuracy: 0.9192 - val_loss: 1.2779 - val_accuracy: 0.6655\n",
            "Epoch 28/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2544 - accuracy: 0.9179 - val_loss: 1.3031 - val_accuracy: 0.6655\n",
            "Epoch 29/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2206 - accuracy: 0.9359 - val_loss: 1.2555 - val_accuracy: 0.6690\n",
            "Epoch 30/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2413 - accuracy: 0.9248 - val_loss: 1.2831 - val_accuracy: 0.6655\n",
            "Epoch 31/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2264 - accuracy: 0.9335 - val_loss: 1.3781 - val_accuracy: 0.6937\n",
            "Epoch 32/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2368 - accuracy: 0.9160 - val_loss: 1.4189 - val_accuracy: 0.6761\n",
            "Epoch 33/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2159 - accuracy: 0.9279 - val_loss: 1.3617 - val_accuracy: 0.7042\n",
            "Epoch 34/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.2010 - accuracy: 0.9397 - val_loss: 1.4017 - val_accuracy: 0.7007\n",
            "Epoch 35/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.1952 - accuracy: 0.9372 - val_loss: 1.3767 - val_accuracy: 0.6655\n",
            "Epoch 36/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.1758 - accuracy: 0.9434 - val_loss: 1.3298 - val_accuracy: 0.7042\n",
            "Epoch 37/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.1554 - accuracy: 0.9440 - val_loss: 1.3601 - val_accuracy: 0.6796\n",
            "Epoch 38/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.1396 - accuracy: 0.9527 - val_loss: 1.4159 - val_accuracy: 0.6972\n",
            "Epoch 39/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.1532 - accuracy: 0.9534 - val_loss: 1.4641 - val_accuracy: 0.6866\n",
            "Epoch 40/40\n",
            "1608/1608 [==============================] - 27s 17ms/step - loss: 0.1672 - accuracy: 0.9447 - val_loss: 1.4501 - val_accuracy: 0.6831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_5ymd8genUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84b8a5d4-022b-4c94-ab0c-6be4ec54b1dc"
      },
      "source": [
        "give_accuracy(lstm_model, padded_test, y_test, \"LSTM\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of LSTM model --->  0.7014925373134329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoPKgyrm1alH",
        "colab_type": "text"
      },
      "source": [
        "#### **I even tried Stacked LSTMs but there was not much improvement in accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtQGRDyWA6-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.save_weights('/content/drive/My Drive/model_weights/model_lstm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udgq9WsIvb_K",
        "colab_type": "code",
        "outputId": "76323f67-cde9-4554-da05-1906dd7c6df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(hist.history['accuracy'],label = 'train_accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], label = 'validation_accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3tNweWKJUlf",
        "colab_type": "text"
      },
      "source": [
        "# ***3. Bidirectional LSTM***\n",
        "\n",
        "- The intuition behind using Bidirectional LSTMs is that they learn contextual information from both directions. This may help model to predict better outputs for text classification\n",
        "![alt text](https://github.com/mananm98/Reddit-Flair-Predictor/blob/master/images/Bidirectional_Lstm.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uyEkClHJT0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix, vocab_len) = create_data_for_model(max_len = 35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5u0P2GnJTyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_biLSTM_model(max_len,vocab_size):\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=True,weights = [embeddings_matrix],trainable = True)(inp1)\n",
        "  drop_out1 = Dropout(0.3)(embedding_layer)\n",
        "  lstm_layer1 = Bidirectional(LSTM(100,return_sequences = False))(drop_out1)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  model_bi_lstm = Model(inputs = inp1,outputs = out1)\n",
        "  model_bi_lstm.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model_bi_lstm.summary()\n",
        "  return model_bi_lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p547SbAn8r64",
        "colab_type": "code",
        "outputId": "eec5e741-8739-48ed-b44a-52c708efc66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model_bi_lstm = create_biLSTM_model(max_len = 35, vocab_size = vocab_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 35)                0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 35, 50)            1018350   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 35, 50)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 200)               120800    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 11)                2211      \n",
            "=================================================================\n",
            "Total params: 1,141,361\n",
            "Trainable params: 1,141,361\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISCt44C4C8E5",
        "colab_type": "code",
        "outputId": "c99101dc-f617-47f0-82a4-09f03e34f0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model_bi_lstm.fit(padded_train,y_train,epochs = 30, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/30\n",
            "1608/1608 [==============================] - 55s 34ms/step - loss: 1.7822 - accuracy: 0.4148 - val_loss: 1.2580 - val_accuracy: 0.6444\n",
            "Epoch 2/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 1.1549 - accuracy: 0.6381 - val_loss: 1.0739 - val_accuracy: 0.6761\n",
            "Epoch 3/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.8327 - accuracy: 0.7345 - val_loss: 0.9790 - val_accuracy: 0.7042\n",
            "Epoch 4/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.6350 - accuracy: 0.8053 - val_loss: 0.9219 - val_accuracy: 0.7077\n",
            "Epoch 5/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.4999 - accuracy: 0.8495 - val_loss: 0.9935 - val_accuracy: 0.6937\n",
            "Epoch 6/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.3776 - accuracy: 0.8744 - val_loss: 0.9403 - val_accuracy: 0.7430\n",
            "Epoch 7/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.2934 - accuracy: 0.9080 - val_loss: 1.0081 - val_accuracy: 0.7148\n",
            "Epoch 8/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.2445 - accuracy: 0.9223 - val_loss: 1.1022 - val_accuracy: 0.6972\n",
            "Epoch 9/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.1964 - accuracy: 0.9391 - val_loss: 1.0316 - val_accuracy: 0.7148\n",
            "Epoch 10/30\n",
            "1608/1608 [==============================] - 52s 33ms/step - loss: 0.1654 - accuracy: 0.9534 - val_loss: 0.9866 - val_accuracy: 0.7183\n",
            "Epoch 11/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.1270 - accuracy: 0.9633 - val_loss: 1.0543 - val_accuracy: 0.7289\n",
            "Epoch 12/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.1330 - accuracy: 0.9590 - val_loss: 1.0788 - val_accuracy: 0.7148\n",
            "Epoch 13/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.1107 - accuracy: 0.9652 - val_loss: 1.0501 - val_accuracy: 0.7289\n",
            "Epoch 14/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.1292 - accuracy: 0.9583 - val_loss: 1.0966 - val_accuracy: 0.7324\n",
            "Epoch 15/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.0921 - accuracy: 0.9689 - val_loss: 1.1277 - val_accuracy: 0.7183\n",
            "Epoch 16/30\n",
            "1608/1608 [==============================] - 54s 34ms/step - loss: 0.0712 - accuracy: 0.9739 - val_loss: 1.1780 - val_accuracy: 0.7183\n",
            "Epoch 17/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.0742 - accuracy: 0.9726 - val_loss: 1.2675 - val_accuracy: 0.7183\n",
            "Epoch 18/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.0726 - accuracy: 0.9739 - val_loss: 1.1228 - val_accuracy: 0.7359\n",
            "Epoch 19/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.0669 - accuracy: 0.9739 - val_loss: 1.1873 - val_accuracy: 0.7359\n",
            "Epoch 20/30\n",
            "1608/1608 [==============================] - 53s 33ms/step - loss: 0.0675 - accuracy: 0.9726 - val_loss: 1.1371 - val_accuracy: 0.7500\n",
            "Epoch 21/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.0625 - accuracy: 0.9739 - val_loss: 1.2250 - val_accuracy: 0.7324\n",
            "Epoch 22/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.0618 - accuracy: 0.9757 - val_loss: 1.2293 - val_accuracy: 0.7218\n",
            "Epoch 23/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.0634 - accuracy: 0.9751 - val_loss: 1.2027 - val_accuracy: 0.7218\n",
            "Epoch 24/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.0727 - accuracy: 0.9683 - val_loss: 1.2841 - val_accuracy: 0.7183\n",
            "Epoch 25/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.0583 - accuracy: 0.9764 - val_loss: 1.2018 - val_accuracy: 0.7254\n",
            "Epoch 26/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.0622 - accuracy: 0.9789 - val_loss: 1.2511 - val_accuracy: 0.7218\n",
            "Epoch 27/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.0618 - accuracy: 0.9720 - val_loss: 1.2138 - val_accuracy: 0.7113\n",
            "Epoch 28/30\n",
            "1608/1608 [==============================] - 51s 32ms/step - loss: 0.0524 - accuracy: 0.9733 - val_loss: 1.2773 - val_accuracy: 0.7218\n",
            "Epoch 29/30\n",
            "1608/1608 [==============================] - 54s 33ms/step - loss: 0.0543 - accuracy: 0.9776 - val_loss: 1.3102 - val_accuracy: 0.7218\n",
            "Epoch 30/30\n",
            "1608/1608 [==============================] - 52s 32ms/step - loss: 0.0538 - accuracy: 0.9757 - val_loss: 1.2768 - val_accuracy: 0.7394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FVC83cg9VS1",
        "colab_type": "code",
        "outputId": "2646f6ce-01fd-47c5-949f-8db44abc71e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = model_bi_lstm.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)\n",
        "# 0.7731343283582089 best saved"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7402985074626866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeTXId80Lara",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bi_lstm.save_weights('/content/drive/My Drive/model_weights/model_bi_lstm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmM9BRmpEC8G",
        "colab_type": "text"
      },
      "source": [
        "# **5. Hybrid  - CNN + LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26tWGg6Dx1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(padded_train, y_train, padded_test, y_test, embeddings_matrix,vocab_len) = create_data_for_model(max_len = 35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0oyH3YETkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_hybrid_model(max_len,vocab_size):\n",
        "\n",
        "  pooling_layers = []\n",
        "\n",
        "\n",
        "  inp1 = Input((max_len,))\n",
        "  embedding_layer = Embedding(input_dim=vocab_size + 1,output_dim= 50,mask_zero=False,weights = [embeddings_matrix],trainable = True)(inp1)\n",
        "  dp = Dropout(0.3)(embedding_layer)\n",
        "\n",
        "  x1 = Conv1D(filters=36,kernel_size=1,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 9,stride = 1)(x1))\n",
        "\n",
        "  x2 = Conv1D(filters=36,kernel_size=2,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 10,stride = 1)(x2))\n",
        "\n",
        "  x3 = Conv1D(filters=36,kernel_size=3,activation = 'relu')(dp)\n",
        "  pooling_layers.append(MaxPool1D(pool_size=max_len - 11,stride = 1)(x3))\n",
        "\n",
        "  z = Concatenate(axis = 2)(pooling_layers)\n",
        "\n",
        "  lstm_layer1 = (LSTM(100,return_sequences = False))(z)\n",
        "\n",
        "\n",
        "  #drop_out = Dropout(0.3)(lstm_layer1)\n",
        "  out1 = Dense(11,activation='softmax')(lstm_layer1)\n",
        "\n",
        "  hybrid_model = Model(inputs = inp1,outputs = out1)\n",
        "  hybrid_model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  hybrid_model.summary()\n",
        "  return hybrid_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WGkqPgcHS00",
        "colab_type": "code",
        "outputId": "b26b2f6d-2141-4ea4-82f8-9d71f0c1d948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hybrid_model = create_hybrid_model(35,vocab_size=vocab_len)\n",
        "hist = hybrid_model.fit(padded_train,y_train,epochs = 35, batch_size=8,validation_split = 0.15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=26, strides=1)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=25, strides=1)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=24, strides=1)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 35)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 35, 50)       1018350     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 35, 50)       0           embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 35, 36)       1836        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 34, 36)       3636        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 33, 36)       5436        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 10, 36)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 10, 36)       0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 10, 36)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 10, 108)      0           max_pooling1d_4[0][0]            \n",
            "                                                                 max_pooling1d_5[0][0]            \n",
            "                                                                 max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 100)          83600       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 11)           1111        lstm_6[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,113,969\n",
            "Trainable params: 1,113,969\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1608 samples, validate on 284 samples\n",
            "Epoch 1/35\n",
            "1608/1608 [==============================] - 8s 5ms/step - loss: 2.3007 - accuracy: 0.1704 - val_loss: 2.1316 - val_accuracy: 0.2782\n",
            "Epoch 2/35\n",
            "1608/1608 [==============================] - 7s 5ms/step - loss: 1.9987 - accuracy: 0.3197 - val_loss: 1.6954 - val_accuracy: 0.4683\n",
            "Epoch 3/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 1.5932 - accuracy: 0.4795 - val_loss: 1.4011 - val_accuracy: 0.5458\n",
            "Epoch 4/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 1.2931 - accuracy: 0.5939 - val_loss: 1.2121 - val_accuracy: 0.5810\n",
            "Epoch 5/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 1.0814 - accuracy: 0.6654 - val_loss: 1.1148 - val_accuracy: 0.6338\n",
            "Epoch 6/35\n",
            "1608/1608 [==============================] - 8s 5ms/step - loss: 0.9045 - accuracy: 0.7195 - val_loss: 1.0415 - val_accuracy: 0.6514\n",
            "Epoch 7/35\n",
            "1608/1608 [==============================] - 8s 5ms/step - loss: 0.7727 - accuracy: 0.7606 - val_loss: 1.0894 - val_accuracy: 0.6725\n",
            "Epoch 8/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.6706 - accuracy: 0.7923 - val_loss: 0.9833 - val_accuracy: 0.6620\n",
            "Epoch 9/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.5900 - accuracy: 0.8141 - val_loss: 1.0753 - val_accuracy: 0.6796\n",
            "Epoch 10/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.5354 - accuracy: 0.8315 - val_loss: 1.0599 - val_accuracy: 0.6725\n",
            "Epoch 11/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.4613 - accuracy: 0.8445 - val_loss: 1.0780 - val_accuracy: 0.6937\n",
            "Epoch 12/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.4186 - accuracy: 0.8576 - val_loss: 1.1110 - val_accuracy: 0.6761\n",
            "Epoch 13/35\n",
            "1608/1608 [==============================] - 7s 5ms/step - loss: 0.3633 - accuracy: 0.8825 - val_loss: 1.1406 - val_accuracy: 0.6725\n",
            "Epoch 14/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.3031 - accuracy: 0.9017 - val_loss: 1.1674 - val_accuracy: 0.6937\n",
            "Epoch 15/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.2677 - accuracy: 0.9192 - val_loss: 1.1411 - val_accuracy: 0.6901\n",
            "Epoch 16/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.2544 - accuracy: 0.9154 - val_loss: 1.1889 - val_accuracy: 0.6796\n",
            "Epoch 17/35\n",
            "1608/1608 [==============================] - 7s 5ms/step - loss: 0.2199 - accuracy: 0.9291 - val_loss: 1.1847 - val_accuracy: 0.6937\n",
            "Epoch 18/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1828 - accuracy: 0.9428 - val_loss: 1.2166 - val_accuracy: 0.6761\n",
            "Epoch 19/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1754 - accuracy: 0.9447 - val_loss: 1.3566 - val_accuracy: 0.6585\n",
            "Epoch 20/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1863 - accuracy: 0.9465 - val_loss: 1.2227 - val_accuracy: 0.7113\n",
            "Epoch 21/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1774 - accuracy: 0.9434 - val_loss: 1.2445 - val_accuracy: 0.6901\n",
            "Epoch 22/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1590 - accuracy: 0.9471 - val_loss: 1.2561 - val_accuracy: 0.7113\n",
            "Epoch 23/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1256 - accuracy: 0.9571 - val_loss: 1.2785 - val_accuracy: 0.7007\n",
            "Epoch 24/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1226 - accuracy: 0.9627 - val_loss: 1.3450 - val_accuracy: 0.6866\n",
            "Epoch 25/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1166 - accuracy: 0.9602 - val_loss: 1.3046 - val_accuracy: 0.6972\n",
            "Epoch 26/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0986 - accuracy: 0.9677 - val_loss: 1.3397 - val_accuracy: 0.6937\n",
            "Epoch 27/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.1251 - accuracy: 0.9552 - val_loss: 1.3982 - val_accuracy: 0.6725\n",
            "Epoch 28/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0944 - accuracy: 0.9627 - val_loss: 1.3400 - val_accuracy: 0.7077\n",
            "Epoch 29/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0859 - accuracy: 0.9714 - val_loss: 1.3966 - val_accuracy: 0.6831\n",
            "Epoch 30/35\n",
            "1608/1608 [==============================] - 7s 5ms/step - loss: 0.0855 - accuracy: 0.9670 - val_loss: 1.2720 - val_accuracy: 0.7113\n",
            "Epoch 31/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0849 - accuracy: 0.9714 - val_loss: 1.3823 - val_accuracy: 0.6972\n",
            "Epoch 32/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0744 - accuracy: 0.9757 - val_loss: 1.4773 - val_accuracy: 0.7042\n",
            "Epoch 33/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0931 - accuracy: 0.9677 - val_loss: 1.3971 - val_accuracy: 0.6937\n",
            "Epoch 34/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0857 - accuracy: 0.9695 - val_loss: 1.3730 - val_accuracy: 0.6937\n",
            "Epoch 35/35\n",
            "1608/1608 [==============================] - 7s 4ms/step - loss: 0.0766 - accuracy: 0.9720 - val_loss: 1.3672 - val_accuracy: 0.6831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mANRef43O-DD",
        "colab_type": "code",
        "outputId": "f5e76000-36f1-4877-e9ec-f271f3a1c96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = hybrid_model.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(\"accuracy of hybrid_model ---> \", acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of hybrid_model --->  0.7134328358208956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BigsyxeWbKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hybrid_model.save_weights('/content/drive/My Drive/model_weights/model_bi_lstm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjJ9EIB4WmKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYh2BWvNmojH",
        "colab_type": "code",
        "outputId": "1d45cbe7-b847-43cf-f848-af03d1de0426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = hybrid_model2.predict(padded_test)\n",
        "predicted_labels = np.argmax(predicted,axis = 1)\n",
        "acc = np.sum(predicted_labels == y_test)/len(y_test)\n",
        "print(acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6626865671641791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8UWdBTpMEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}